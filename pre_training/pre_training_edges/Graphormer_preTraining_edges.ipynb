{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andrea-1704/Pytorch_Geometric_tutorial/blob/main/train_model_baseline_f1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiCB_SVeanwO"
      },
      "source": [
        "In questo netebook proviamo ad implementare un processo di **pre gtraining** basato su self supervision tramite un task di edge prediction. L'obiettivo sarà quello di migliorare la capacità cognitiva dei dettagli strutturali della rete neurale. In pratica andremo a rimuovere dal grafo alcuni archi o aggiungerne altri e miglioreremo la capacità della rete nel riconoscere gli archi falsi e quelli mancanti."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "zNziUzq9nTdU"
      },
      "outputs": [],
      "source": [
        "# !pip install torch-scatter -f https://data.pyg.org/whl/torch-2.4.0+cpu.html\n",
        "# !pip install torch-sparse -f https://data.pyg.org/whl/torch-2.4.0+cpu.html\n",
        "# !pip install torch-cluster -f https://data.pyg.org/whl/torch-2.4.0+cpu.html\n",
        "# !pip install torch-spline-conv -f https://data.pyg.org/whl/torch-2.4.0+cpu.html\n",
        "# !pip install torch-geometric==2.6.0 -f https://data.pyg.org/whl/torch-2.4.0+cpu.html\n",
        "# !pip install pyg-lib -f https://data.pyg.org/whl/torch-2.4.0+cpu.html\n",
        "\n",
        "# !pip install pytorch_frame[full]==1.2.2\n",
        "# !pip install relbench[full]==1.0.0\n",
        "# !pip uninstall -y pyg_lib torch  # Uninstall current versions\n",
        "# !pip install torch==2.6.0  # Reinstall your desired PyTorch version\n",
        "# !pip install --no-cache-dir git+https://github.com/pyg-team/pyg-lib.git # Install pyg-lib; --no-cache-dir ensures a fresh install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8VinWw-anwR"
      },
      "source": [
        "New libraries to run on colab:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "cTB7Q6qQanwR"
      },
      "outputs": [],
      "source": [
        "# !pip install torch==2.6.0+cu118 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# !pip install pyg-lib -f https://data.pyg.org/whl/torch-2.6.0+cu118.html\n",
        "# !pip install torch-scatter -f https://data.pyg.org/whl/torch-2.6.0+cu118.html\n",
        "# !pip install torch-sparse -f https://data.pyg.org/whl/torch-2.6.0+cu118.html\n",
        "# !pip install torch-cluster -f https://data.pyg.org/whl/torch-2.6.0+cu118.html\n",
        "# !pip install torch-spline-conv -f https://data.pyg.org/whl/torch-2.6.0+cu118.html\n",
        "# !pip install torch-geometric==2.6.0 -f https://data.pyg.org/whl/torch-2.6.0+cu118.html\n",
        "\n",
        "# !pip install pytorch_frame[full]==1.2.2\n",
        "# !pip install relbench[full]==1.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "F454ta1Zg0Oq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import relbench\n",
        "import numpy as np\n",
        "from torch.nn import BCEWithLogitsLoss, L1Loss\n",
        "from relbench.datasets import get_dataset\n",
        "from relbench.tasks import get_task\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import torch_geometric\n",
        "import torch_frame\n",
        "from torch_geometric.seed import seed_everything\n",
        "from relbench.modeling.utils import get_stype_proposal\n",
        "from collections import defaultdict\n",
        "import requests\n",
        "from io import StringIO\n",
        "from torch_frame.config.text_embedder import TextEmbedderConfig\n",
        "from relbench.modeling.graph import make_pkey_fkey_graph\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "import copy\n",
        "from typing import Any, Dict, List\n",
        "from torch import Tensor\n",
        "from torch.nn import Embedding, ModuleDict\n",
        "from torch_frame.data.stats import StatType\n",
        "from torch_geometric.data import HeteroData\n",
        "from torch_geometric.nn import MLP\n",
        "from torch_geometric.typing import NodeType\n",
        "from relbench.modeling.nn import HeteroEncoder, HeteroGraphSAGE, HeteroTemporalEncoder\n",
        "from relbench.modeling.graph import get_node_train_table_input, make_pkey_fkey_graph\n",
        "from torch_geometric.loader import NeighborLoader\n",
        "import pyg_lib\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#per lo scheduler\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch_geometric.nn import Linear\n",
        "from torch_geometric.utils import softmax\n",
        "from torch_geometric.utils import degree\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTytIKXvanwT"
      },
      "source": [
        "# Dataset and task creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DWB-Kf6nl2y",
        "outputId": "7607806a-aeee-4f4f-d9f8-eff64b1cf073"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "dataset = get_dataset(\"rel-f1\", download=True)\n",
        "task = get_task(\"rel-f1\", \"driver-position\", download=True)\n",
        "\n",
        "train_table = task.get_table(\"train\")\n",
        "val_table = task.get_table(\"val\")\n",
        "test_table = task.get_table(\"test\")\n",
        "\n",
        "out_channels = 1\n",
        "# one because we are estimating one single value.\n",
        "loss_fn = L1Loss()\n",
        "# this is the mae loss and is used when have regressions tasks.\n",
        "tune_metric = \"mae\"\n",
        "higher_is_better = False\n",
        "\n",
        "seed_everything(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "root_dir = \"./data\"\n",
        "\n",
        "db = dataset.get_db()\n",
        "col_to_stype_dict = get_stype_proposal(db)\n",
        "#this is used to get the stype of the columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVsgNYpuanwU"
      },
      "source": [
        "# Embedder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "QQHYmgIxkX1j"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# from typing import List, Optional\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "# from torch import Tensor\n",
        "\n",
        "\n",
        "# class GloveTextEmbedding:\n",
        "#     def __init__(self, device: Optional[torch.device\n",
        "#                                        ] = None):\n",
        "#         self.model = SentenceTransformer(\n",
        "#             \"sentence-transformers/average_word_embeddings_glove.6B.300d\",\n",
        "#             device=device,\n",
        "#         )\n",
        "\n",
        "#     def __call__(self, sentences: List[str]) -> Tensor:\n",
        "#         return torch.from_numpy(self.model.encode(sentences))\n",
        "\n",
        "\n",
        "class LightweightGloveEmbedder:\n",
        "    def __init__(self, device=None):\n",
        "        self.device = device\n",
        "        self.embeddings = defaultdict(lambda: np.zeros(300))\n",
        "        self._load_embeddings()\n",
        "\n",
        "    def _load_embeddings(self):\n",
        "        try:\n",
        "            #(senza bisogno di estrarre zip\n",
        "            url = \"https://huggingface.co/stanfordnlp/glove/resolve/main/glove.6B.300d.txt\"\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            for line in StringIO(response.text):\n",
        "                parts = line.split()\n",
        "                word = parts[0]\n",
        "                vector = np.array(parts[1:], dtype=np.float32)\n",
        "                self.embeddings[word] = vector\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Couldn't load GloVe embeddings ({str(e)}). Using zero vectors.\")\n",
        "\n",
        "    def __call__(self, sentences):\n",
        "        results = []\n",
        "        for text in sentences:\n",
        "            words = text.lower().split()\n",
        "            vectors = [self.embeddings[w] for w in words if w in self.embeddings]\n",
        "            if vectors:\n",
        "                avg_vector = np.mean(vectors, axis=0)\n",
        "            else:\n",
        "                avg_vector = np.zeros(300)\n",
        "            results.append(avg_vector)\n",
        "\n",
        "        tensor = torch.tensor(np.array(results), dtype=torch.float32)\n",
        "        return tensor.to(self.device) if self.device else tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-BBpUrakdwY",
        "outputId": "657811d2-852c-4746-ebbd-8383739f8297"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Couldn't load GloVe embeddings (404 Client Error: Not Found for url: https://huggingface.co/stanfordnlp/glove/resolve/main/glove.6B.300d.txt). Using zero vectors.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_frame/utils/io.py:113: UserWarning: Weights only load failed. Please file an issue to make `torch.load(weights_only=True)` compatible in your case. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global.\n",
            "  warnings.warn(f\"{warn_msg} Please use \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_frame/utils/io.py:113: UserWarning: Weights only load failed. Please file an issue to make `torch.load(weights_only=True)` compatible in your case. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global.\n",
            "  warnings.warn(f\"{warn_msg} Please use \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_frame/utils/io.py:113: UserWarning: Weights only load failed. Please file an issue to make `torch.load(weights_only=True)` compatible in your case. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global.\n",
            "  warnings.warn(f\"{warn_msg} Please use \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_frame/utils/io.py:113: UserWarning: Weights only load failed. Please file an issue to make `torch.load(weights_only=True)` compatible in your case. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global.\n",
            "  warnings.warn(f\"{warn_msg} Please use \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_frame/utils/io.py:113: UserWarning: Weights only load failed. Please file an issue to make `torch.load(weights_only=True)` compatible in your case. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global.\n",
            "  warnings.warn(f\"{warn_msg} Please use \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_frame/utils/io.py:113: UserWarning: Weights only load failed. Please file an issue to make `torch.load(weights_only=True)` compatible in your case. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global.\n",
            "  warnings.warn(f\"{warn_msg} Please use \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_frame/utils/io.py:113: UserWarning: Weights only load failed. Please file an issue to make `torch.load(weights_only=True)` compatible in your case. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global.\n",
            "  warnings.warn(f\"{warn_msg} Please use \"\n"
          ]
        }
      ],
      "source": [
        "text_embedder_cfg = TextEmbedderConfig(\n",
        "    text_embedder=LightweightGloveEmbedder(device=device), batch_size=256\n",
        ")\n",
        "\n",
        "data, col_stats_dict = make_pkey_fkey_graph(\n",
        "    #Solution if not working: !pip install --upgrade torch torchvision transformers\n",
        "    db,\n",
        "    col_to_stype_dict=col_to_stype_dict,  # speficied column types\n",
        "    text_embedder_cfg=text_embedder_cfg,  # our chosen text encoder\n",
        "    cache_dir=os.path.join(\n",
        "        root_dir, f\"rel-f1_materialized_cache\"\n",
        "    ),  # store materialized graph for convenience\n",
        ")# create a graph how relbench requires."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "HUHVG-g6lM-b"
      },
      "outputs": [],
      "source": [
        "loader_dict = {}\n",
        "\n",
        "for split, table in [\n",
        "    (\"train\", train_table),\n",
        "    (\"val\", val_table),\n",
        "    (\"test\", test_table),\n",
        "]:\n",
        "    table_input = get_node_train_table_input(\n",
        "        table=table,\n",
        "        task=task,\n",
        "    )#notice that table_input is an object with three elements: nodes, time and transform.\n",
        "    #nodes contains the input nodes\n",
        "    #time contains the time for each node\n",
        "    #transform is the tranformation to be applied to nodes\n",
        "    entity_table = table_input.nodes[0]\n",
        "    #we need to populate the loader_dict with three elements: \"train\", \"val\", and \"test\".\n",
        "    loader_dict[split] = NeighborLoader(\n",
        "        data,\n",
        "        num_neighbors=[\n",
        "            128 for i in range(2)\n",
        "        ],  # we sample subgraphs of depth 2, 128 neighbors per node.\n",
        "        time_attr=\"time\",\n",
        "        input_nodes=table_input.nodes,\n",
        "        input_time=table_input.time,\n",
        "        transform=table_input.transform,\n",
        "        batch_size=512,\n",
        "        temporal_strategy=\"uniform\",\n",
        "        shuffle=split == \"train\",\n",
        "        num_workers=0,\n",
        "        persistent_workers=False,\n",
        "    )#this is the loader for grapg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNecfPt3anwW"
      },
      "source": [
        "Nota che ogni oggetto \"batch\" rappresenta un dizionario in cui le chiavi rappresentano i tipi di nodi oppure i tipi di archi e contengono gli embedding del nodo opppure l'edge index dell'arco (vale a dire un tensore di dimensione 2xnumero di edge, ovvero la lista dei nodi di tipo sorgente che presdentano una relazione di quel tipo con i nodi di tipo destinazione, ovvero quelli della seconda lista)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7DiIz69anwW"
      },
      "source": [
        "# Pre-training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZxabXSOanwW"
      },
      "outputs": [],
      "source": [
        "def sample_edge_prediction_batch(batch: HeteroData, edge_type, neg_ratio=1.0):\n",
        "\n",
        "    #prendiamo gli archi positivi, ovvero quelli che esistono davvero:\n",
        "    pos_edge_index = batch[edge_type].edge_index\n",
        "    num_pos = pos_edge_index.size(1)\n",
        "\n",
        "    if num_pos == 0:\n",
        "        #se non abbiamo archi positivie saltimao il batch\n",
        "        raise ValueError(f\"No positive edges found in batch for edge_type {edge_type}\")\n",
        "\n",
        "    #prendiamo ora i sample negativi: archi che non esistono nel grafo\n",
        "    src, dst = pos_edge_index[0], pos_edge_index[1]\n",
        "    neg_src = src[torch.randint(0, src.size(0), (int(num_pos * neg_ratio),))]\n",
        "    neg_dst = dst[torch.randint(0, dst.size(0), (int(num_pos * neg_ratio),))]\n",
        "    #nota potremmo averere come negativi degli archi che in realtà sono positivi\n",
        "    pos_labels = torch.ones(num_pos, device=src.device)\n",
        "    neg_labels = torch.zeros(neg_src.size(0), device=src.device)\n",
        "\n",
        "    edge_src = torch.cat([src, neg_src], dim=0)\n",
        "    edge_dst = torch.cat([dst, neg_dst], dim=0)\n",
        "    labels = torch.cat([pos_labels, neg_labels], dim=0)\n",
        "\n",
        "    return edge_src, edge_dst, labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Restituisce edge_src che è il tensore degli id dei nodi sorgenti per gli archi positivi e per quelli negativi; edge_dst che è il tensore degli archi destinazione sia positivi che negativi e labels che è un tensore di uni e zeri per dire se positivo o negativo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b16yUmmaanwX"
      },
      "source": [
        "Adeso andiamo ad implementare un modello che verrà utilizzato per effettuare il processo di **edge prediction**, ovvero il pre training al fine di inizializzare in maniera \"intelligente\" i pesi della rete."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQYKrvcWanwX"
      },
      "source": [
        "Adesso necessitiamo di definire un metodo che effettui il processo di pre training, sarebbe quindi come definire un codice alternativo di training per un task differente (quello di link prediction) questo modello verà poi raffinato tramite un processo di trainig per il downstream task (quello effettivo di relbench)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Xh7RauOmanwY"
      },
      "outputs": [],
      "source": [
        "def pretrain_edge_prediction(model, loader, edge_type, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "    #non lavora male per il task che dobbiamo effettuare sugli archi visto\n",
        "    #che effettuiamo un task di predizione binaria (link presente o meno)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in loader:\n",
        "            batch = batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logits, labels = model(batch, edge_type)\n",
        "            loss = loss_fn(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch}, Loss: {total_loss / len(loader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3AfboisanwY"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "PoAmtoPsanwZ"
      },
      "outputs": [],
      "source": [
        "_spatial_bias_cache = None\n",
        "_node_offset_cache = None\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def compute_spatial_bias(edge_index_dict, x_dict):\n",
        "    global _spatial_bias_cache, _node_offset_cache\n",
        "    if _spatial_bias_cache is not None:\n",
        "        return _spatial_bias_cache, _node_offset_cache\n",
        "    #creiamo un grafo diretto con Networkx\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    node_offset = {}\n",
        "    curr_offset = 0\n",
        "\n",
        "    #aggiungiamo i nodi con offset per mantenere indici globali univoci\n",
        "    for node_type, x in x_dict.items():\n",
        "        node_offset[node_type] = curr_offset\n",
        "        for i in range(x.size(0)):\n",
        "            G.add_node(curr_offset + i, type=node_type)\n",
        "        curr_offset += x.size(0)\n",
        "\n",
        "    #Aggiungiamo gli archi con offset\n",
        "    for (src_type, _, dst_type), edge_index in edge_index_dict.items():\n",
        "        src_offset = node_offset[src_type]\n",
        "        dst_offset = node_offset[dst_type]\n",
        "        src, dst = edge_index\n",
        "        for s, d in zip(src.tolist(), dst.tolist()):\n",
        "            G.add_edge(src_offset + s, dst_offset + d)\n",
        "\n",
        "\n",
        "    spatial_bias = defaultdict(lambda: -1)\n",
        "\n",
        "\n",
        "\n",
        "    for node in G.nodes():\n",
        "        lengths = nx.single_source_dijkstra_path_length(G, node)\n",
        "        for target, dist in lengths.items():\n",
        "            spatial_bias[(node, target)] = dist\n",
        "        #quelli non raggiungibili li lasciamo con default value, ovvero -1\n",
        "\n",
        "    _spatial_bias_cache = spatial_bias\n",
        "    _node_offset_cache = node_offset\n",
        "\n",
        "    return spatial_bias, node_offset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "rN-8kKQianwa"
      },
      "outputs": [],
      "source": [
        "class HeteroGraphormerLayerComplete(nn.Module):\n",
        "    def __init__(self, channels, edge_types, device, num_heads=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.num_heads = num_heads\n",
        "        self.channels = channels\n",
        "        self.head_dim = channels // num_heads\n",
        "\n",
        "        assert self.channels % num_heads == 0, \"channels must be divisible by num_heads\"\n",
        "\n",
        "        self.q_lin = Linear(channels, channels)\n",
        "        self.k_lin = Linear(channels, channels)\n",
        "        self.v_lin = Linear(channels, channels)\n",
        "        self.out_lin = Linear(channels, channels)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm = nn.LayerNorm(channels)\n",
        "\n",
        "        # Registriamo i bias per ogni tipo di edge nel __init__\n",
        "        self.edge_type_bias = nn.ParameterDict({\n",
        "            \"__\".join(edge_type): nn.Parameter(torch.randn(1))\n",
        "            for edge_type in edge_types\n",
        "        })\n",
        "\n",
        "    def compute_total_degrees(self, x_dict, edge_index_dict):\n",
        "        device = self.device\n",
        "        in_deg = defaultdict(lambda: torch.zeros(0, device=device))\n",
        "        out_deg = defaultdict(lambda: torch.zeros(0, device=device))\n",
        "        for edge_type, edge_index in edge_index_dict.items():\n",
        "            src_type, _, dst_type = edge_type\n",
        "            src = edge_index[0]\n",
        "            dst = edge_index[1]\n",
        "\n",
        "            num_src = x_dict[src_type].size(0)\n",
        "            num_dst = x_dict[dst_type].size(0)\n",
        "\n",
        "            if out_deg[src_type].numel() == 0:\n",
        "                out_deg[src_type] = torch.zeros(num_src, device=device)\n",
        "            if in_deg[dst_type].numel() == 0:\n",
        "                in_deg[dst_type] = torch.zeros(num_dst, device=device)\n",
        "\n",
        "            out_deg[src_type] += degree(src, num_nodes=num_src)\n",
        "            in_deg[dst_type]  += degree(dst, num_nodes=num_dst)\n",
        "\n",
        "        total_deg = {\n",
        "            node_type: in_deg[node_type] + out_deg[node_type]\n",
        "            for node_type in x_dict\n",
        "        }\n",
        "\n",
        "        return total_deg\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict):\n",
        "        #print(edge_index_dict)\n",
        "        self.spatial_bias, self.node_offset = compute_spatial_bias(edge_index_dict, x_dict)\n",
        "\n",
        "        out_dict = {k: torch.zeros_like(v) for k, v in x_dict.items()}\n",
        "        for edge_type, edge_index in edge_index_dict.items():\n",
        "            src_type, _, dst_type = edge_type\n",
        "            x_src, x_dst = x_dict[src_type], x_dict[dst_type]\n",
        "\n",
        "            #src, dst = edge_index\n",
        "            src = edge_index[0]\n",
        "            dst = edge_index[1]\n",
        "\n",
        "            Q = self.q_lin(x_dst).view(-1, self.num_heads, self.head_dim)\n",
        "            K = self.k_lin(x_src).view(-1, self.num_heads, self.head_dim)\n",
        "            V = self.v_lin(x_src).view(-1, self.num_heads, self.head_dim)\n",
        "\n",
        "            attn_scores = (Q[dst] * K[src]).sum(dim=-1) / self.head_dim**0.5\n",
        "            src_offset = self.node_offset[src_type]\n",
        "            dst_offset = self.node_offset[dst_type]\n",
        "\n",
        "            spatial_bias_vals = []\n",
        "            for s, d in zip(src.tolist(), dst.tolist()):\n",
        "                global_s = src_offset + s\n",
        "                global_d = dst_offset + d\n",
        "                dist = self.spatial_bias.get((global_d, global_s), -1.0)\n",
        "                spatial_bias_vals.append(dist)\n",
        "\n",
        "            spatial_bias_tensor = torch.tensor(spatial_bias_vals, dtype=torch.float, device=self.device)\n",
        "            attn_scores = attn_scores + spatial_bias_tensor.unsqueeze(-1)  # broadcast su heads\n",
        "\n",
        "\n",
        "            bias_name = \"__\".join(edge_type)\n",
        "            attn_scores = attn_scores + self.edge_type_bias[bias_name]\n",
        "\n",
        "            attn_weights = softmax(attn_scores, dst)\n",
        "            attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "            out = V[src] * attn_weights.unsqueeze(-1)\n",
        "            out = out.view(-1, self.channels)\n",
        "\n",
        "            out_dict[dst_type].index_add_(0, dst, out)\n",
        "\n",
        "        #calcolo della degree centrality\n",
        "\n",
        "        total_deg = self.compute_total_degrees(x_dict, edge_index_dict)\n",
        "\n",
        "\n",
        "        for node_type in out_dict:\n",
        "\n",
        "            degree_embed = total_deg[node_type].view(-1, 1)                                                                                  # Assicurati che sia una colonna\n",
        "            degree_embed = degree_embed.expand(-1, self.channels)                                                                            # Espandi lungo la dimensione dei canali\n",
        "\n",
        "\n",
        "            out_dict[node_type] = out_dict[node_type] + degree_embed\n",
        "\n",
        "\n",
        "        for node_type in out_dict:\n",
        "            out_dict[node_type] = self.norm(out_dict[node_type] + x_dict[node_type])\n",
        "\n",
        "        return out_dict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-ARUEdQanwa"
      },
      "source": [
        "## HeteroGraphormer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "xz8Y3N_kanwa"
      },
      "outputs": [],
      "source": [
        "class HeteroGraphormer(torch.nn.Module):\n",
        "    def __init__(self, node_types, edge_types, channels, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.layers = torch.nn.ModuleList([\n",
        "            HeteroGraphormerLayerComplete(channels, edge_types, device) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict, *args, **kwargs):\n",
        "        for layer in self.layers:\n",
        "            x_dict = layer(x_dict, edge_index_dict)\n",
        "        return x_dict\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for layer in self.layers:\n",
        "            if hasattr(layer, \"reset_parameters\"):\n",
        "                layer.reset_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "u3m3jEqClQnw"
      },
      "outputs": [],
      "source": [
        "class Model(torch.nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        data: HeteroData, #notice that \"data2 is the graph we created with function make_pkey_fkey_graph\n",
        "        col_stats_dict: Dict[str, Dict[str, Dict[StatType, Any]]],\n",
        "        num_layers: int,\n",
        "        channels: int,\n",
        "        out_channels: int,\n",
        "        aggr: str,\n",
        "        norm: str,\n",
        "        # List of node types to add shallow embeddings to input\n",
        "        shallow_list: List[NodeType] = [],\n",
        "        # ID awareness\n",
        "        id_awareness: bool = False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = HeteroEncoder(\n",
        "            channels=channels,\n",
        "            node_to_col_names_dict={\n",
        "                node_type: data[node_type].tf.col_names_dict\n",
        "                for node_type in data.node_types\n",
        "            },\n",
        "            node_to_col_stats=col_stats_dict,\n",
        "        )\n",
        "        self.temporal_encoder = HeteroTemporalEncoder(\n",
        "            node_types=[\n",
        "                node_type for node_type in data.node_types if \"time\" in data[node_type]\n",
        "            ],\n",
        "            channels=channels,\n",
        "        )\n",
        "        self.gnn = HeteroGraphormer(\n",
        "            node_types=data.node_types,\n",
        "            edge_types=data.edge_types,\n",
        "            channels=channels,\n",
        "            num_layers=num_layers,\n",
        "        )\n",
        "        self.head = MLP(\n",
        "            channels,#one, since we are doing regression\n",
        "            out_channels=out_channels,\n",
        "            norm=norm,\n",
        "            num_layers=1,\n",
        "        )\n",
        "        self.embedding_dict = ModuleDict(\n",
        "            {\n",
        "                node: Embedding(data.num_nodes_dict[node], channels)\n",
        "                for node in shallow_list\n",
        "            }\n",
        "        )\n",
        "\n",
        "        self.id_awareness_emb = None\n",
        "        if id_awareness:\n",
        "            self.id_awareness_emb = torch.nn.Embedding(1, channels)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.encoder.reset_parameters()\n",
        "        self.temporal_encoder.reset_parameters()\n",
        "        self.gnn.reset_parameters()\n",
        "        self.head.reset_parameters()\n",
        "        for embedding in self.embedding_dict.values():\n",
        "            torch.nn.init.normal_(embedding.weight, std=0.1)\n",
        "        if self.id_awareness_emb is not None:\n",
        "            self.id_awareness_emb.reset_parameters()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        batch: HeteroData,\n",
        "        entity_table: NodeType,\n",
        "    ) -> Tensor:\n",
        "        seed_time = batch[entity_table].seed_time\n",
        "        #takes the timestamp of the nodes for which we want to make predictions\n",
        "        #not the neighbours, but the nodes we want to make prediction for.\n",
        "        x_dict = self.encoder(batch.tf_dict)\n",
        "        #this creates a dictionar for all the nodes: each nodes has its\n",
        "        #embedding\n",
        "\n",
        "        rel_time_dict = self.temporal_encoder(\n",
        "            seed_time, batch.time_dict, batch.batch_dict\n",
        "        )\n",
        "        #this add the temporal information to the node using the\n",
        "        #HeteroTemporalEncoder\n",
        "\n",
        "        for node_type, rel_time in rel_time_dict.items():\n",
        "            x_dict[node_type] = x_dict[node_type] + rel_time\n",
        "        #add some other shallow embedder\n",
        "\n",
        "        for node_type, embedding in self.embedding_dict.items():\n",
        "            x_dict[node_type] = x_dict[node_type] + embedding(batch[node_type].n_id)\n",
        "\n",
        "        # for edge_type, edge_index in batch.edge_index_dict.items():\n",
        "        #     print(\"model edge_tipe: \", edge_type)\n",
        "        #     print(\"model edge_index: \", edge_index)\n",
        "        #print(\"model x_dict : \", x_dict['constructors'])\n",
        "\n",
        "        x_dict = self.gnn(\n",
        "            x_dict,#feature of nodes\n",
        "            batch.edge_index_dict,\n",
        "            batch.num_sampled_nodes_dict,\n",
        "            batch.num_sampled_edges_dict,\n",
        "        )#apply the gnn\n",
        "\n",
        "        return self.head(x_dict[entity_table][: seed_time.size(0)])#final prediction\n",
        "\n",
        "    def forward_dst_readout(\n",
        "        self,\n",
        "        batch: HeteroData,\n",
        "        entity_table: NodeType,\n",
        "        dst_table: NodeType,\n",
        "    ) -> Tensor:\n",
        "        if self.id_awareness_emb is None:\n",
        "            raise RuntimeError(\n",
        "                \"id_awareness must be set True to use forward_dst_readout\"\n",
        "            )\n",
        "        seed_time = batch[entity_table].seed_time\n",
        "        x_dict = self.encoder(batch.tf_dict)\n",
        "        # Add ID-awareness to the root node\n",
        "        x_dict[entity_table][: seed_time.size(0)] += self.id_awareness_emb.weight\n",
        "\n",
        "        rel_time_dict = self.temporal_encoder(\n",
        "            seed_time, batch.time_dict, batch.batch_dict\n",
        "        )\n",
        "\n",
        "        for node_type, rel_time in rel_time_dict.items():\n",
        "            x_dict[node_type] = x_dict[node_type] + rel_time\n",
        "\n",
        "        for node_type, embedding in self.embedding_dict.items():\n",
        "            x_dict[node_type] = x_dict[node_type] + embedding(batch[node_type].n_id)\n",
        "\n",
        "        x_dict = self.gnn(\n",
        "            x_dict,\n",
        "            batch.edge_index_dict,\n",
        "        )\n",
        "\n",
        "        return self.head(x_dict[dst_table])\n",
        "\n",
        "\n",
        "model = Model(\n",
        "    data=data,\n",
        "    col_stats_dict=col_stats_dict,\n",
        "    num_layers=2,\n",
        "    channels=128,\n",
        "    out_channels=1,\n",
        "    aggr=\"sum\",\n",
        "    norm=\"batch_norm\",\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pre training Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skoIYQJYby7l"
      },
      "outputs": [],
      "source": [
        "class EdgePredictionModel(torch.nn.Module):\n",
        "    def __init__(self, base_model: Model):\n",
        "        super().__init__()\n",
        "        self.model=base_model\n",
        "        self.encoder = base_model.encoder\n",
        "        self.temporal_encoder = base_model.temporal_encoder\n",
        "        self.gnn = base_model.gnn\n",
        "\n",
        "    def encode(self, batch: HeteroData) -> Dict[str, Tensor]:\n",
        "        x_dict = self.encoder(batch.tf_dict)\n",
        "        x_dict = self.gnn(x_dict, batch.edge_index_dict)\n",
        "        return x_dict\n",
        "    #ottiene gli embeddings dei nodi\n",
        "\n",
        "    def decode(self, src_emb, dst_emb):\n",
        "        return (src_emb * dst_emb).sum(dim=1)  #dot product, calcolo similarity\n",
        "    #riceve gli embedding dei nodi sorgente e destinazione e calcola il prodotto \n",
        "    # scalare (dot product) tra le coppie → misura di similarità.\n",
        "    # Questo è un logit che può essere passato a una BCEWithLogitsLoss \n",
        "    # per predizione binaria (edge presente o assente).\n",
        "\n",
        "    def forward(self, batch: HeteroData, edge_type):\n",
        "        x_dict = self.encode(batch)\n",
        "\n",
        "        src, dst, labels = sample_edge_prediction_batch(batch, edge_type)\n",
        "\n",
        "        src_emb = x_dict[edge_type[0]][src]#embeddings dei nodi sorgente\n",
        "        dst_emb = x_dict[edge_type[2]][dst]#embeddings dei nodi destinazione\n",
        "        logits = self.decode(src_emb, dst_emb)\n",
        "\n",
        "        return logits, labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MaL0--Sanwb"
      },
      "source": [
        "# Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "K87Ya4wPanwc"
      },
      "outputs": [],
      "source": [
        "def get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps, num_cycles=0.5):\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < warmup_steps:\n",
        "            return float(current_step) / float(max(1, warmup_steps))\n",
        "        progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
        "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * num_cycles * 2 * progress)))\n",
        "\n",
        "    return LambdaLR(optimizer, lr_lambda)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "SAHRIr15lVs6"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, scheduler) -> float:\n",
        "    model.train()\n",
        "\n",
        "    loss_accum = count_accum = 0\n",
        "    for batch in tqdm(loader_dict[\"train\"]):\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(\n",
        "            batch,\n",
        "            task.entity_table,\n",
        "        )\n",
        "        pred = pred.view(-1) if pred.size(1) == 1 else pred\n",
        "\n",
        "        loss = loss_fn(pred.float(), batch[entity_table].y.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        loss_accum += loss.detach().item() * pred.size(0)\n",
        "        count_accum += pred.size(0)\n",
        "\n",
        "    return loss_accum / count_accum\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, loader: NeighborLoader) -> np.ndarray:\n",
        "    model.eval()\n",
        "\n",
        "    pred_list = []\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        pred = model(\n",
        "            batch,\n",
        "            task.entity_table,\n",
        "        )\n",
        "        pred = pred.view(-1) if pred.size(1) == 1 else pred\n",
        "        pred_list.append(pred.detach().cpu())\n",
        "    return torch.cat(pred_list, dim=0).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "d0lcTAwAanwc"
      },
      "outputs": [],
      "source": [
        "def rmse(true, pred):\n",
        "    \"\"\"Calculate the Root Mean Squared Error (RMSE).\"\"\"\n",
        "    return np.sqrt(np.mean((true - pred)**2)) # Calculate RMSE manually"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "qZOOyAblHwI4"
      },
      "outputs": [],
      "source": [
        "def custom_evaluate(pred: np.ndarray, target_table, metrics) -> dict:\n",
        "    \"\"\"Custom evaluation function to replace task.evaluate.\"\"\"\n",
        "\n",
        "    # Extract target values from the target table\n",
        "    target = target_table.df[task.target_col].to_numpy()\n",
        "\n",
        "    # Check for length mismatch\n",
        "    if len(pred) != len(target):\n",
        "        raise ValueError(\n",
        "            f\"The length of pred and target must be the same (got \"\n",
        "            f\"{len(pred)} and {len(target)}, respectively).\"\n",
        "        )\n",
        "\n",
        "    # Calculate metrics\n",
        "    results = {}\n",
        "    for metric_fn in metrics:\n",
        "        if metric_fn.__name__ == \"rmse\":  # Handle RMSE specifically\n",
        "            results[\"rmse\"] = np.sqrt(np.mean((target - pred)**2))\n",
        "        else:  # Handle other metrics (if any)\n",
        "            results[metric_fn.__name__] = metric_fn(target, pred)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "KMn-f8dmanwc"
      },
      "outputs": [],
      "source": [
        "def training_function(model, optimizer, epochs):\n",
        "    state_dict = None\n",
        "    best_val_metric = -math.inf if higher_is_better else math.inf\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_loss = train(model, optimizer)\n",
        "        val_pred = test(model, loader_dict[\"val\"])\n",
        "        #val_metrics = task.evaluate(val_pred, val_table)\n",
        "        val_metrics = custom_evaluate(val_pred, val_table, task.metrics)\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch: {epoch:02d}, Train loss: {train_loss}, Val metrics: {val_metrics}\")\n",
        "        #print(f\"Epoch: {epoch:02d}, Train loss: {train_loss}, Val metrics: {val_metrics}\")\n",
        "\n",
        "        if (higher_is_better and val_metrics[tune_metric] > best_val_metric) or (\n",
        "            not higher_is_better and val_metrics[tune_metric] < best_val_metric\n",
        "        ):\n",
        "            best_val_metric = val_metrics[tune_metric]\n",
        "            state_dict = copy.deepcopy(model.state_dict())\n",
        "\n",
        "\n",
        "    model.load_state_dict(state_dict)\n",
        "    val_pred = test(model, loader_dict[\"val\"])\n",
        "    val_metrics = custom_evaluate(val_pred, val_table, task.metrics)\n",
        "    print(f\"Best Val metrics for parameters {optimizer}, are: {val_metrics}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpFIqgwTanwd"
      },
      "source": [
        "## Cross validation cycle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "qLUoqMtranwd"
      },
      "outputs": [],
      "source": [
        "# #cross validation cycle:\n",
        "# #possible learning rates: [0.01, 0.001, 0.0001, 0.00001]\n",
        "# #possible batch sizes: [64, 256, 512]\n",
        "# #possible number of layers: [1, 2, 3]\n",
        "# #possible weight decay: [0.0001, 0.001, 0.01]\n",
        "\n",
        "# for lr in [0.01, 0.001, 0.0001, 0.00001]:#0.001\n",
        "#     #for batch_size in [64, 256, 512]:\n",
        "#         for num_layers in [1, 2, 3]:#1\n",
        "#             #for weight_decay in [0.0001, 0.001, 0.01]:\n",
        "#                 model = Model(\n",
        "#                     data=data,\n",
        "#                     col_stats_dict=col_stats_dict,\n",
        "#                     num_layers=num_layers,\n",
        "#                     channels=128,\n",
        "#                     out_channels=1,\n",
        "#                     aggr=\"sum\",\n",
        "#                     norm=\"batch_norm\",\n",
        "#                 ).to(device)\n",
        "#                 print(f\"Training with lr={lr}, num_layers={num_layers}\")\n",
        "#                 optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.001)\n",
        "#                 training_function(model, optimizer, epochs=10) # Set epochs to a smaller number for testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_RxdxPHanwd"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "lbPEXYnranwe"
      },
      "outputs": [],
      "source": [
        "def plot_validation_metrics(metric_histories, model_names=None, metric_name=\"MAE\", informationsTitle=\"\"):\n",
        "    \"\"\"\n",
        "    Plotta l'andamento del metric_name per più modelli nel tempo.\n",
        "\n",
        "    Args:\n",
        "        metric_histories (list of lists): Lista di liste, ognuna rappresenta i valori di metriche per un modello.\n",
        "        model_names (list of str): Nomi dei modelli (opzionale).\n",
        "        metric_name (str): Nome della metrica da visualizzare.\n",
        "        informationsTitle (str): info aggiungitive da mettere nel titolo (conf generale dei parametri ecc).\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(9, 5))\n",
        "\n",
        "    if model_names is None:\n",
        "        model_names = [f\"Model {i+1}\" for i in range(len(metric_histories))]\n",
        "\n",
        "    for metrics, name in zip(metric_histories, model_names):\n",
        "        plt.plot(metrics, marker='o', label=f'{name} {metric_name}')\n",
        "\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(metric_name)\n",
        "    plt.title(f\"{metric_name} over Epochs for Multiple Models {informationsTitle}\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yF3W68Eqlew_",
        "outputId": "6091dfd5-3b8f-4040-ee49-9f6055061f60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Pretraining per edge_type: ('races', 'rev_f2p_raceId', 'standings')\n",
            "Epoch 0, Loss: 4.3043\n",
            "Epoch 1, Loss: 0.7483\n",
            "Epoch 2, Loss: 0.3625\n",
            "Epoch 3, Loss: 0.2449\n",
            "Epoch 4, Loss: 0.1915\n",
            "Epoch 5, Loss: 0.1621\n",
            "Epoch 6, Loss: 0.1416\n",
            "Epoch 7, Loss: 0.1264\n",
            "Epoch 8, Loss: 0.1128\n",
            "Epoch 9, Loss: 0.1028\n",
            "Epoch 10, Loss: 0.0958\n",
            "Epoch 11, Loss: 0.0899\n",
            "Epoch 12, Loss: 0.0835\n",
            "Epoch 13, Loss: 0.0783\n",
            "Epoch 14, Loss: 0.0754\n",
            "Epoch 15, Loss: 0.0717\n",
            "Epoch 16, Loss: 0.0681\n",
            "Epoch 17, Loss: 0.0656\n",
            "Epoch 18, Loss: 0.0635\n",
            "Epoch 19, Loss: 0.0609\n",
            "Epoch 20, Loss: 0.0573\n",
            "Epoch 21, Loss: 0.0554\n",
            "Epoch 22, Loss: 0.0530\n",
            "Epoch 23, Loss: 0.0498\n",
            "Epoch 24, Loss: 0.0472\n",
            "Epoch 25, Loss: 0.0438\n",
            "Epoch 26, Loss: 0.0426\n",
            "Epoch 27, Loss: 0.0400\n",
            "Epoch 28, Loss: 0.0392\n",
            "Epoch 29, Loss: 0.0363\n",
            "Epoch 30, Loss: 0.0343\n",
            "Epoch 31, Loss: 0.0328\n",
            "Epoch 32, Loss: 0.0312\n",
            "Epoch 33, Loss: 0.0301\n",
            "Epoch 34, Loss: 0.0295\n",
            "Epoch 35, Loss: 0.0279\n",
            "Epoch 36, Loss: 0.0267\n",
            "Epoch 37, Loss: 0.0255\n",
            "Epoch 38, Loss: 0.0247\n",
            "Epoch 39, Loss: 0.0243\n",
            "Epoch 40, Loss: 0.0232\n",
            "Epoch 41, Loss: 0.0223\n",
            "Epoch 42, Loss: 0.0215\n",
            "Epoch 43, Loss: 0.0212\n",
            "Epoch 44, Loss: 0.0203\n",
            "Epoch 45, Loss: 0.0199\n",
            "Epoch 46, Loss: 0.0188\n",
            "Epoch 47, Loss: 0.0181\n",
            "Epoch 48, Loss: 0.0179\n",
            "Epoch 49, Loss: 0.0175\n",
            "\n",
            "Pretraining per edge_type: ('standings', 'f2p_driverId', 'drivers')\n",
            "Epoch 0, Loss: 5.8751\n",
            "Epoch 1, Loss: 1.6494\n",
            "Epoch 2, Loss: 0.9326\n",
            "Epoch 3, Loss: 0.7047\n",
            "Epoch 4, Loss: 0.6181\n",
            "Epoch 5, Loss: 0.5612\n",
            "Epoch 6, Loss: 0.5092\n",
            "Epoch 7, Loss: 0.4218\n",
            "Epoch 8, Loss: 0.3349\n",
            "Epoch 9, Loss: 0.2682\n",
            "Epoch 10, Loss: 0.2209\n",
            "Epoch 11, Loss: 0.1874\n",
            "Epoch 12, Loss: 0.1728\n",
            "Epoch 13, Loss: 0.1455\n",
            "Epoch 14, Loss: 0.1179\n",
            "Epoch 15, Loss: 0.1093\n",
            "Epoch 16, Loss: 0.0939\n",
            "Epoch 17, Loss: 0.0891\n",
            "Epoch 18, Loss: 0.0826\n",
            "Epoch 19, Loss: 0.0773\n",
            "Epoch 20, Loss: 0.0727\n",
            "Epoch 21, Loss: 0.0675\n",
            "Epoch 22, Loss: 0.0657\n",
            "Epoch 23, Loss: 0.0602\n",
            "Epoch 24, Loss: 0.0615\n",
            "Epoch 25, Loss: 0.0592\n",
            "Epoch 26, Loss: 0.0570\n",
            "Epoch 27, Loss: 0.0535\n",
            "Epoch 28, Loss: 0.0515\n",
            "Epoch 29, Loss: 0.0524\n",
            "Epoch 30, Loss: 0.0496\n",
            "Epoch 31, Loss: 0.0461\n",
            "Epoch 32, Loss: 0.0450\n",
            "Epoch 33, Loss: 0.0449\n",
            "Epoch 34, Loss: 0.0477\n",
            "Epoch 35, Loss: 0.0462\n",
            "Epoch 36, Loss: 0.0451\n",
            "Epoch 37, Loss: 0.0441\n",
            "Epoch 38, Loss: 0.0404\n",
            "Epoch 39, Loss: 0.0415\n",
            "Epoch 40, Loss: 0.0397\n",
            "Epoch 41, Loss: 0.0390\n",
            "Epoch 42, Loss: 0.0379\n",
            "Epoch 43, Loss: 0.0382\n",
            "Epoch 44, Loss: 0.0377\n",
            "Epoch 45, Loss: 0.0363\n",
            "Epoch 46, Loss: 0.0363\n",
            "Epoch 47, Loss: 0.0357\n",
            "Epoch 48, Loss: 0.0354\n",
            "Epoch 49, Loss: 0.0343\n",
            "\n",
            "Pretraining per edge_type: ('drivers', 'rev_f2p_driverId', 'standings')\n",
            "Epoch 0, Loss: 3.9935\n",
            "Epoch 1, Loss: 0.9427\n",
            "Epoch 2, Loss: 0.6652\n",
            "Epoch 3, Loss: 0.5712\n",
            "Epoch 4, Loss: 0.4926\n",
            "Epoch 5, Loss: 0.4310\n",
            "Epoch 6, Loss: 0.3733\n",
            "Epoch 7, Loss: 0.2975\n",
            "Epoch 8, Loss: 0.2239\n",
            "Epoch 9, Loss: 0.1733\n",
            "Epoch 10, Loss: 0.1452\n",
            "Epoch 11, Loss: 0.1218\n",
            "Epoch 12, Loss: 0.1060\n",
            "Epoch 13, Loss: 0.0977\n",
            "Epoch 14, Loss: 0.0881\n",
            "Epoch 15, Loss: 0.0825\n",
            "Epoch 16, Loss: 0.0767\n",
            "Epoch 17, Loss: 0.0739\n",
            "Epoch 18, Loss: 0.0681\n",
            "Epoch 19, Loss: 0.0637\n",
            "Epoch 20, Loss: 0.0609\n",
            "Epoch 21, Loss: 0.0594\n",
            "Epoch 22, Loss: 0.0557\n",
            "Epoch 23, Loss: 0.0551\n",
            "Epoch 24, Loss: 0.0539\n",
            "Epoch 25, Loss: 0.0490\n",
            "Epoch 26, Loss: 0.0483\n",
            "Epoch 27, Loss: 0.0471\n",
            "Epoch 28, Loss: 0.0457\n",
            "Epoch 29, Loss: 0.0440\n",
            "Epoch 30, Loss: 0.0440\n",
            "Epoch 31, Loss: 0.0420\n",
            "Epoch 32, Loss: 0.0440\n",
            "Epoch 33, Loss: 0.0418\n",
            "Epoch 34, Loss: 0.0398\n",
            "Epoch 35, Loss: 0.0377\n",
            "Epoch 36, Loss: 0.0371\n",
            "Epoch 37, Loss: 0.0384\n",
            "Epoch 38, Loss: 0.0375\n",
            "Epoch 39, Loss: 0.0352\n",
            "Epoch 40, Loss: 0.0352\n",
            "Epoch 41, Loss: 0.0327\n",
            "Epoch 42, Loss: 0.0340\n",
            "Epoch 43, Loss: 0.0378\n",
            "Epoch 44, Loss: 0.0345\n",
            "Epoch 45, Loss: 0.0332\n",
            "Epoch 46, Loss: 0.0331\n",
            "Epoch 47, Loss: 0.0330\n",
            "Epoch 48, Loss: 0.0313\n",
            "Epoch 49, Loss: 0.0304\n",
            "\n",
            "Pretraining per edge_type: ('races', 'rev_f2p_raceId', 'results')\n",
            "Epoch 0, Loss: 6.5995\n",
            "Epoch 1, Loss: 1.2755\n",
            "Epoch 2, Loss: 0.6187\n",
            "Epoch 3, Loss: 0.4175\n",
            "Epoch 4, Loss: 0.3343\n",
            "Epoch 5, Loss: 0.2817\n",
            "Epoch 6, Loss: 0.2441\n",
            "Epoch 7, Loss: 0.2151\n",
            "Epoch 8, Loss: 0.1905\n",
            "Epoch 9, Loss: 0.1686\n",
            "Epoch 10, Loss: 0.1527\n",
            "Epoch 11, Loss: 0.1379\n",
            "Epoch 12, Loss: 0.1242\n",
            "Epoch 13, Loss: 0.1147\n",
            "Epoch 14, Loss: 0.1063\n",
            "Epoch 15, Loss: 0.0990\n",
            "Epoch 16, Loss: 0.0938\n",
            "Epoch 17, Loss: 0.0875\n",
            "Epoch 18, Loss: 0.0846\n",
            "Epoch 19, Loss: 0.0801\n",
            "Epoch 20, Loss: 0.0775\n",
            "Epoch 21, Loss: 0.0737\n",
            "Epoch 22, Loss: 0.0701\n",
            "Epoch 23, Loss: 0.0679\n",
            "Epoch 24, Loss: 0.0668\n",
            "Epoch 25, Loss: 0.0629\n",
            "Epoch 26, Loss: 0.0591\n",
            "Epoch 27, Loss: 0.0567\n",
            "Epoch 28, Loss: 0.0549\n",
            "Epoch 29, Loss: 0.0526\n",
            "Epoch 30, Loss: 0.0496\n",
            "Epoch 31, Loss: 0.0485\n",
            "Epoch 32, Loss: 0.0464\n",
            "Epoch 33, Loss: 0.0444\n",
            "Epoch 34, Loss: 0.0426\n",
            "Epoch 35, Loss: 0.0409\n",
            "Epoch 36, Loss: 0.0402\n",
            "Epoch 37, Loss: 0.0385\n",
            "Epoch 38, Loss: 0.0369\n",
            "Epoch 39, Loss: 0.0363\n",
            "Epoch 40, Loss: 0.0346\n",
            "Epoch 41, Loss: 0.0337\n",
            "Epoch 42, Loss: 0.0328\n",
            "Epoch 43, Loss: 0.0320\n",
            "Epoch 44, Loss: 0.0314\n",
            "Epoch 45, Loss: 0.0306\n",
            "Epoch 46, Loss: 0.0290\n",
            "Epoch 47, Loss: 0.0288\n",
            "Epoch 48, Loss: 0.0282\n",
            "Epoch 49, Loss: 0.0274\n",
            "\n",
            "Pretraining per edge_type: ('results', 'f2p_driverId', 'drivers')\n",
            "Epoch 0, Loss: 9.5091\n",
            "Epoch 1, Loss: 2.1203\n",
            "Epoch 2, Loss: 1.0840\n",
            "Epoch 3, Loss: 0.7743\n",
            "Epoch 4, Loss: 0.6718\n",
            "Epoch 5, Loss: 0.6324\n",
            "Epoch 6, Loss: 0.5998\n",
            "Epoch 7, Loss: 0.5879\n",
            "Epoch 8, Loss: 0.5676\n",
            "Epoch 9, Loss: 0.5438\n",
            "Epoch 10, Loss: 0.5048\n",
            "Epoch 11, Loss: 0.4582\n",
            "Epoch 12, Loss: 0.4017\n",
            "Epoch 13, Loss: 0.3436\n",
            "Epoch 14, Loss: 0.2938\n",
            "Epoch 15, Loss: 0.2543\n",
            "Epoch 16, Loss: 0.2153\n",
            "Epoch 17, Loss: 0.1935\n",
            "Epoch 18, Loss: 0.1798\n",
            "Epoch 19, Loss: 0.1607\n",
            "Epoch 20, Loss: 0.1420\n",
            "Epoch 21, Loss: 0.1290\n",
            "Epoch 22, Loss: 0.1203\n",
            "Epoch 23, Loss: 0.1120\n",
            "Epoch 24, Loss: 0.1045\n",
            "Epoch 25, Loss: 0.0942\n",
            "Epoch 26, Loss: 0.0918\n",
            "Epoch 27, Loss: 0.0873\n",
            "Epoch 28, Loss: 0.0797\n",
            "Epoch 29, Loss: 0.0788\n",
            "Epoch 30, Loss: 0.0744\n",
            "Epoch 31, Loss: 0.0730\n",
            "Epoch 32, Loss: 0.0709\n",
            "Epoch 33, Loss: 0.0689\n",
            "Epoch 34, Loss: 0.0649\n",
            "Epoch 35, Loss: 0.0630\n",
            "Epoch 36, Loss: 0.0637\n",
            "Epoch 37, Loss: 0.0602\n",
            "Epoch 38, Loss: 0.0576\n",
            "Epoch 39, Loss: 0.0555\n",
            "Epoch 40, Loss: 0.0539\n",
            "Epoch 41, Loss: 0.0559\n",
            "Epoch 42, Loss: 0.0561\n",
            "Epoch 43, Loss: 0.0522\n",
            "Epoch 44, Loss: 0.0493\n",
            "Epoch 45, Loss: 0.0499\n",
            "Epoch 46, Loss: 0.0509\n",
            "Epoch 47, Loss: 0.0480\n",
            "Epoch 48, Loss: 0.0468\n",
            "Epoch 49, Loss: 0.0470\n",
            "\n",
            "Pretraining per edge_type: ('drivers', 'rev_f2p_driverId', 'results')\n",
            "Epoch 0, Loss: 8.7527\n",
            "Epoch 1, Loss: 2.1757\n",
            "Epoch 2, Loss: 1.0046\n",
            "Epoch 3, Loss: 0.7400\n",
            "Epoch 4, Loss: 0.6536\n",
            "Epoch 5, Loss: 0.6198\n",
            "Epoch 6, Loss: 0.5937\n",
            "Epoch 7, Loss: 0.5742\n",
            "Epoch 8, Loss: 0.5547\n",
            "Epoch 9, Loss: 0.5275\n",
            "Epoch 10, Loss: 0.4825\n",
            "Epoch 11, Loss: 0.4047\n",
            "Epoch 12, Loss: 0.3252\n",
            "Epoch 13, Loss: 0.2976\n",
            "Epoch 14, Loss: 0.2350\n",
            "Epoch 15, Loss: 0.2046\n",
            "Epoch 16, Loss: 0.1670\n",
            "Epoch 17, Loss: 0.1502\n",
            "Epoch 18, Loss: 0.1335\n",
            "Epoch 19, Loss: 0.1237\n",
            "Epoch 20, Loss: 0.1113\n",
            "Epoch 21, Loss: 0.1082\n",
            "Epoch 22, Loss: 0.0989\n",
            "Epoch 23, Loss: 0.0946\n",
            "Epoch 24, Loss: 0.0865\n",
            "Epoch 25, Loss: 0.0821\n",
            "Epoch 26, Loss: 0.0804\n",
            "Epoch 27, Loss: 0.0761\n",
            "Epoch 28, Loss: 0.0744\n",
            "Epoch 29, Loss: 0.0672\n",
            "Epoch 30, Loss: 0.0646\n",
            "Epoch 31, Loss: 0.0619\n",
            "Epoch 32, Loss: 0.0613\n",
            "Epoch 33, Loss: 0.0597\n",
            "Epoch 34, Loss: 0.0586\n",
            "Epoch 35, Loss: 0.0568\n",
            "Epoch 36, Loss: 0.0553\n",
            "Epoch 37, Loss: 0.0544\n",
            "Epoch 38, Loss: 0.0518\n",
            "Epoch 39, Loss: 0.0490\n",
            "Epoch 40, Loss: 0.0498\n",
            "Epoch 41, Loss: 0.0474\n",
            "Epoch 42, Loss: 0.0510\n",
            "Epoch 43, Loss: 0.0489\n",
            "Epoch 44, Loss: 0.0457\n",
            "Epoch 45, Loss: 0.0448\n",
            "Epoch 46, Loss: 0.0445\n",
            "Epoch 47, Loss: 0.0448\n",
            "Epoch 48, Loss: 0.0464\n",
            "Epoch 49, Loss: 0.0450\n",
            "\n",
            "Pretraining per edge_type: ('constructors', 'rev_f2p_constructorId', 'results')\n",
            "Epoch 0, Loss: 6.7895\n",
            "Epoch 1, Loss: 1.4880\n",
            "Epoch 2, Loss: 0.8795\n",
            "Epoch 3, Loss: 0.7352\n",
            "Epoch 4, Loss: 0.6782\n",
            "Epoch 5, Loss: 0.6403\n",
            "Epoch 6, Loss: 0.6026\n",
            "Epoch 7, Loss: 0.5451\n",
            "Epoch 8, Loss: 0.4671\n",
            "Epoch 9, Loss: 0.3782\n",
            "Epoch 10, Loss: 0.2860\n",
            "Epoch 11, Loss: 0.2292\n",
            "Epoch 12, Loss: 0.1913\n",
            "Epoch 13, Loss: 0.1601\n",
            "Epoch 14, Loss: 0.1404\n",
            "Epoch 15, Loss: 0.1230\n",
            "Epoch 16, Loss: 0.1076\n",
            "Epoch 17, Loss: 0.0961\n",
            "Epoch 18, Loss: 0.0871\n",
            "Epoch 19, Loss: 0.0815\n",
            "Epoch 20, Loss: 0.0749\n",
            "Epoch 21, Loss: 0.0678\n",
            "Epoch 22, Loss: 0.0689\n",
            "Epoch 23, Loss: 0.0617\n",
            "Epoch 24, Loss: 0.0591\n",
            "Epoch 25, Loss: 0.0589\n",
            "Epoch 26, Loss: 0.0556\n",
            "Epoch 27, Loss: 0.0513\n",
            "Epoch 28, Loss: 0.0477\n",
            "Epoch 29, Loss: 0.0483\n",
            "Epoch 30, Loss: 0.0446\n",
            "Epoch 31, Loss: 0.0430\n",
            "Epoch 32, Loss: 0.0414\n",
            "Epoch 33, Loss: 0.0415\n",
            "Epoch 34, Loss: 0.0390\n",
            "Epoch 35, Loss: 0.0390\n",
            "Epoch 36, Loss: 0.0391\n",
            "Epoch 37, Loss: 0.0369\n",
            "Epoch 38, Loss: 0.0377\n",
            "Epoch 39, Loss: 0.0336\n",
            "Epoch 40, Loss: 0.0340\n",
            "Epoch 41, Loss: 0.0349\n",
            "Epoch 42, Loss: 0.0336\n",
            "Epoch 43, Loss: 0.0321\n",
            "Epoch 44, Loss: 0.0300\n",
            "Epoch 45, Loss: 0.0312\n",
            "Epoch 46, Loss: 0.0322\n",
            "Epoch 47, Loss: 0.0308\n",
            "Epoch 48, Loss: 0.0291\n",
            "Epoch 49, Loss: 0.0295\n",
            "\n",
            "Pretraining per edge_type: ('races', 'rev_f2p_raceId', 'qualifying')\n",
            "Epoch 0, Loss: 4.5949\n",
            "Epoch 1, Loss: 0.8932\n",
            "Epoch 2, Loss: 0.4831\n",
            "Epoch 3, Loss: 0.3732\n",
            "Epoch 4, Loss: 0.3353\n",
            "Epoch 5, Loss: 0.3134\n",
            "Epoch 6, Loss: 0.2976\n",
            "Epoch 7, Loss: 0.2883\n",
            "Epoch 8, Loss: 0.2715\n",
            "Epoch 9, Loss: 0.2676\n",
            "Epoch 10, Loss: 0.2576\n",
            "Epoch 11, Loss: 0.2517\n",
            "Epoch 12, Loss: 0.2383\n",
            "Epoch 13, Loss: 0.2302\n",
            "Epoch 14, Loss: 0.2229\n",
            "Epoch 15, Loss: 0.2031\n",
            "Epoch 16, Loss: 0.1844\n",
            "Epoch 17, Loss: 0.1721\n",
            "Epoch 18, Loss: 0.1510\n",
            "Epoch 19, Loss: 0.1399\n",
            "Epoch 20, Loss: 0.1279\n",
            "Epoch 21, Loss: 0.1159\n",
            "Epoch 22, Loss: 0.1057\n",
            "Epoch 23, Loss: 0.1029\n",
            "Epoch 24, Loss: 0.0959\n",
            "Epoch 25, Loss: 0.0891\n",
            "Epoch 26, Loss: 0.0832\n",
            "Epoch 27, Loss: 0.0744\n",
            "Epoch 28, Loss: 0.0708\n",
            "Epoch 29, Loss: 0.0645\n",
            "Epoch 30, Loss: 0.0657\n",
            "Epoch 31, Loss: 0.0620\n",
            "Epoch 32, Loss: 0.0562\n",
            "Epoch 33, Loss: 0.0555\n",
            "Epoch 34, Loss: 0.0525\n",
            "Epoch 35, Loss: 0.0462\n",
            "Epoch 36, Loss: 0.0454\n",
            "Epoch 37, Loss: 0.0430\n",
            "Epoch 38, Loss: 0.0415\n",
            "Epoch 39, Loss: 0.0429\n",
            "Epoch 40, Loss: 0.0415\n",
            "Epoch 41, Loss: 0.0381\n",
            "Epoch 42, Loss: 0.0364\n",
            "Epoch 43, Loss: 0.0359\n",
            "Epoch 44, Loss: 0.0333\n",
            "Epoch 45, Loss: 0.0333\n",
            "Epoch 46, Loss: 0.0316\n",
            "Epoch 47, Loss: 0.0332\n",
            "Epoch 48, Loss: 0.0303\n",
            "Epoch 49, Loss: 0.0314\n",
            "\n",
            "Pretraining per edge_type: ('qualifying', 'f2p_driverId', 'drivers')\n",
            "Epoch 0, Loss: 11.1885\n",
            "Epoch 1, Loss: 2.3896\n",
            "Epoch 2, Loss: 1.2712\n",
            "Epoch 3, Loss: 0.9094\n",
            "Epoch 4, Loss: 0.8278\n",
            "Epoch 5, Loss: 0.7622\n",
            "Epoch 6, Loss: 0.7510\n",
            "Epoch 7, Loss: 0.7358\n",
            "Epoch 8, Loss: 0.7377\n",
            "Epoch 9, Loss: 0.7064\n",
            "Epoch 10, Loss: 0.6732\n",
            "Epoch 11, Loss: 0.6223\n",
            "Epoch 12, Loss: 0.6812\n",
            "Epoch 13, Loss: 0.6686\n",
            "Epoch 14, Loss: 0.5957\n",
            "Epoch 15, Loss: 0.5606\n",
            "Epoch 16, Loss: 0.6319\n",
            "Epoch 17, Loss: 0.6275\n",
            "Epoch 18, Loss: 0.5324\n",
            "Epoch 19, Loss: 0.5064\n",
            "Epoch 20, Loss: 0.4707\n",
            "Epoch 21, Loss: 0.4462\n",
            "Epoch 22, Loss: 0.4766\n",
            "Epoch 23, Loss: 0.3901\n",
            "Epoch 24, Loss: 0.3922\n",
            "Epoch 25, Loss: 0.4148\n",
            "Epoch 26, Loss: 0.3914\n",
            "Epoch 27, Loss: 0.3376\n",
            "Epoch 28, Loss: 0.3655\n",
            "Epoch 29, Loss: 0.3165\n",
            "Epoch 30, Loss: 0.2678\n",
            "Epoch 31, Loss: 0.2790\n",
            "Epoch 32, Loss: 0.2731\n",
            "Epoch 33, Loss: 0.2349\n",
            "Epoch 34, Loss: 0.2320\n",
            "Epoch 35, Loss: 0.2282\n",
            "Epoch 36, Loss: 0.2393\n",
            "Epoch 37, Loss: 0.2231\n",
            "Epoch 38, Loss: 0.2208\n",
            "Epoch 39, Loss: 0.2145\n",
            "Epoch 40, Loss: 0.2922\n",
            "Epoch 41, Loss: 0.2907\n",
            "Epoch 42, Loss: 0.2104\n",
            "Epoch 43, Loss: 0.1803\n",
            "Epoch 44, Loss: 0.1833\n",
            "Epoch 45, Loss: 0.1910\n",
            "Epoch 46, Loss: 0.1695\n",
            "Epoch 47, Loss: 0.1722\n",
            "Epoch 48, Loss: 0.1898\n",
            "Epoch 49, Loss: 0.1864\n",
            "\n",
            "Pretraining per edge_type: ('drivers', 'rev_f2p_driverId', 'qualifying')\n",
            "Epoch 0, Loss: 10.0106\n",
            "Epoch 1, Loss: 1.6474\n",
            "Epoch 2, Loss: 1.0035\n",
            "Epoch 3, Loss: 0.8964\n",
            "Epoch 4, Loss: 0.8848\n",
            "Epoch 5, Loss: 0.8432\n",
            "Epoch 6, Loss: 0.8011\n",
            "Epoch 7, Loss: 0.7698\n",
            "Epoch 8, Loss: 0.7338\n",
            "Epoch 9, Loss: 0.7030\n",
            "Epoch 10, Loss: 0.6876\n",
            "Epoch 11, Loss: 0.6894\n",
            "Epoch 12, Loss: 0.7040\n",
            "Epoch 13, Loss: 0.6574\n",
            "Epoch 14, Loss: 0.6638\n",
            "Epoch 15, Loss: 0.5483\n",
            "Epoch 16, Loss: 0.4894\n",
            "Epoch 17, Loss: 0.4844\n",
            "Epoch 18, Loss: 0.3785\n",
            "Epoch 19, Loss: 0.3495\n",
            "Epoch 20, Loss: 0.3259\n",
            "Epoch 21, Loss: 0.3562\n",
            "Epoch 22, Loss: 0.4223\n",
            "Epoch 23, Loss: 0.3209\n",
            "Epoch 24, Loss: 0.2848\n",
            "Epoch 25, Loss: 0.2655\n",
            "Epoch 26, Loss: 0.2745\n",
            "Epoch 27, Loss: 0.2425\n",
            "Epoch 28, Loss: 0.2362\n",
            "Epoch 29, Loss: 0.2381\n",
            "Epoch 30, Loss: 0.2118\n",
            "Epoch 31, Loss: 0.2179\n",
            "Epoch 32, Loss: 0.2310\n",
            "Epoch 33, Loss: 0.2176\n",
            "Epoch 34, Loss: 0.1969\n",
            "Epoch 35, Loss: 0.1908\n",
            "Epoch 36, Loss: 0.2203\n",
            "Epoch 37, Loss: 0.1950\n",
            "Epoch 38, Loss: 0.1980\n",
            "Epoch 39, Loss: 0.1844\n",
            "Epoch 40, Loss: 0.1840\n",
            "Epoch 41, Loss: 0.1778\n",
            "Epoch 42, Loss: 0.1808\n",
            "Epoch 43, Loss: 0.1954\n",
            "Epoch 44, Loss: 0.1547\n",
            "Epoch 45, Loss: 0.1613\n",
            "Epoch 46, Loss: 0.1683\n",
            "Epoch 47, Loss: 0.1769\n",
            "Epoch 48, Loss: 0.1623\n",
            "Epoch 49, Loss: 0.1716\n",
            "\n",
            "Pretraining per edge_type: ('constructors', 'rev_f2p_constructorId', 'qualifying')\n",
            "Epoch 0, Loss: 6.4569\n",
            "Epoch 1, Loss: 1.3131\n",
            "Epoch 2, Loss: 0.8387\n",
            "Epoch 3, Loss: 0.6970\n",
            "Epoch 4, Loss: 0.6442\n",
            "Epoch 5, Loss: 0.5944\n",
            "Epoch 6, Loss: 0.5450\n",
            "Epoch 7, Loss: 0.4865\n",
            "Epoch 8, Loss: 0.4359\n",
            "Epoch 9, Loss: 0.3790\n",
            "Epoch 10, Loss: 0.3396\n",
            "Epoch 11, Loss: 0.3039\n",
            "Epoch 12, Loss: 0.2714\n",
            "Epoch 13, Loss: 0.2491\n",
            "Epoch 14, Loss: 0.2219\n",
            "Epoch 15, Loss: 0.1918\n",
            "Epoch 16, Loss: 0.1776\n",
            "Epoch 17, Loss: 0.1645\n",
            "Epoch 18, Loss: 0.1638\n",
            "Epoch 19, Loss: 0.1485\n",
            "Epoch 20, Loss: 0.1436\n",
            "Epoch 21, Loss: 0.1407\n",
            "Epoch 22, Loss: 0.1322\n",
            "Epoch 23, Loss: 0.1424\n",
            "Epoch 24, Loss: 0.1318\n",
            "Epoch 25, Loss: 0.1303\n",
            "Epoch 26, Loss: 0.1191\n",
            "Epoch 27, Loss: 0.1138\n",
            "Epoch 28, Loss: 0.1120\n",
            "Epoch 29, Loss: 0.1095\n",
            "Epoch 30, Loss: 0.1132\n",
            "Epoch 31, Loss: 0.0953\n",
            "Epoch 32, Loss: 0.1008\n",
            "Epoch 33, Loss: 0.1035\n",
            "Epoch 34, Loss: 0.0970\n",
            "Epoch 35, Loss: 0.1008\n",
            "Epoch 36, Loss: 0.0999\n",
            "Epoch 37, Loss: 0.0974\n",
            "Epoch 38, Loss: 0.0931\n",
            "Epoch 39, Loss: 0.0894\n",
            "Epoch 40, Loss: 0.0830\n",
            "Epoch 41, Loss: 0.0813\n",
            "Epoch 42, Loss: 0.0911\n",
            "Epoch 43, Loss: 0.0867\n",
            "Epoch 44, Loss: 0.0936\n",
            "Epoch 45, Loss: 0.0922\n",
            "Epoch 46, Loss: 0.0797\n",
            "Epoch 47, Loss: 0.0814\n",
            "Epoch 48, Loss: 0.0777\n",
            "Epoch 49, Loss: 0.0788\n",
            "Parametri modificati: ['encoder.encoders.drivers.encoder.encoder_dict.timestamp.weight', 'encoder.encoders.drivers.encoder.encoder_dict.timestamp.bias', 'encoder.encoders.drivers.encoder.encoder_dict.embedding.biases', 'encoder.encoders.drivers.backbone.0.lin1.weight', 'encoder.encoders.drivers.backbone.0.lin1.bias', 'encoder.encoders.drivers.backbone.0.lin2.weight', 'encoder.encoders.drivers.backbone.0.lin2.bias', 'encoder.encoders.drivers.backbone.0.norm1.weight', 'encoder.encoders.drivers.backbone.0.norm1.bias', 'encoder.encoders.drivers.backbone.0.norm2.weight', 'encoder.encoders.drivers.backbone.0.norm2.bias', 'encoder.encoders.drivers.backbone.0.shortcut.weight', 'encoder.encoders.drivers.backbone.0.shortcut.bias', 'encoder.encoders.drivers.backbone.1.lin1.weight', 'encoder.encoders.drivers.backbone.1.lin1.bias', 'encoder.encoders.drivers.backbone.1.lin2.weight', 'encoder.encoders.drivers.backbone.1.lin2.bias', 'encoder.encoders.drivers.backbone.1.norm1.weight', 'encoder.encoders.drivers.backbone.1.norm1.bias', 'encoder.encoders.drivers.backbone.1.norm2.weight', 'encoder.encoders.drivers.backbone.1.norm2.bias', 'encoder.encoders.drivers.backbone.2.lin1.weight', 'encoder.encoders.drivers.backbone.2.lin1.bias', 'encoder.encoders.drivers.backbone.2.lin2.weight', 'encoder.encoders.drivers.backbone.2.lin2.bias', 'encoder.encoders.drivers.backbone.2.norm1.weight', 'encoder.encoders.drivers.backbone.2.norm1.bias', 'encoder.encoders.drivers.backbone.2.norm2.weight', 'encoder.encoders.drivers.backbone.2.norm2.bias', 'encoder.encoders.drivers.backbone.3.lin1.weight', 'encoder.encoders.drivers.backbone.3.lin1.bias', 'encoder.encoders.drivers.backbone.3.lin2.weight', 'encoder.encoders.drivers.backbone.3.lin2.bias', 'encoder.encoders.drivers.backbone.3.norm1.weight', 'encoder.encoders.drivers.backbone.3.norm1.bias', 'encoder.encoders.drivers.backbone.3.norm2.weight', 'encoder.encoders.drivers.backbone.3.norm2.bias', 'encoder.encoders.drivers.decoder.0.weight', 'encoder.encoders.drivers.decoder.0.bias', 'encoder.encoders.drivers.decoder.2.weight', 'encoder.encoders.drivers.decoder.2.bias', 'encoder.encoders.constructors.encoder.encoder_dict.embedding.biases', 'encoder.encoders.constructors.backbone.0.lin1.weight', 'encoder.encoders.constructors.backbone.0.lin1.bias', 'encoder.encoders.constructors.backbone.0.lin2.weight', 'encoder.encoders.constructors.backbone.0.lin2.bias', 'encoder.encoders.constructors.backbone.0.norm1.weight', 'encoder.encoders.constructors.backbone.0.norm1.bias', 'encoder.encoders.constructors.backbone.0.norm2.weight', 'encoder.encoders.constructors.backbone.0.norm2.bias', 'encoder.encoders.constructors.backbone.0.shortcut.weight', 'encoder.encoders.constructors.backbone.0.shortcut.bias', 'encoder.encoders.constructors.backbone.1.lin1.weight', 'encoder.encoders.constructors.backbone.1.lin1.bias', 'encoder.encoders.constructors.backbone.1.lin2.weight', 'encoder.encoders.constructors.backbone.1.lin2.bias', 'encoder.encoders.constructors.backbone.1.norm1.weight', 'encoder.encoders.constructors.backbone.1.norm1.bias', 'encoder.encoders.constructors.backbone.1.norm2.weight', 'encoder.encoders.constructors.backbone.1.norm2.bias', 'encoder.encoders.constructors.backbone.2.lin1.weight', 'encoder.encoders.constructors.backbone.2.lin1.bias', 'encoder.encoders.constructors.backbone.2.lin2.weight', 'encoder.encoders.constructors.backbone.2.lin2.bias', 'encoder.encoders.constructors.backbone.2.norm1.weight', 'encoder.encoders.constructors.backbone.2.norm1.bias', 'encoder.encoders.constructors.backbone.2.norm2.weight', 'encoder.encoders.constructors.backbone.2.norm2.bias', 'encoder.encoders.constructors.backbone.3.lin1.weight', 'encoder.encoders.constructors.backbone.3.lin1.bias', 'encoder.encoders.constructors.backbone.3.lin2.weight', 'encoder.encoders.constructors.backbone.3.lin2.bias', 'encoder.encoders.constructors.backbone.3.norm1.weight', 'encoder.encoders.constructors.backbone.3.norm1.bias', 'encoder.encoders.constructors.backbone.3.norm2.weight', 'encoder.encoders.constructors.backbone.3.norm2.bias', 'encoder.encoders.constructors.decoder.0.weight', 'encoder.encoders.constructors.decoder.0.bias', 'encoder.encoders.constructors.decoder.2.weight', 'encoder.encoders.constructors.decoder.2.bias', 'encoder.encoders.races.encoder.encoder_dict.categorical.emb.weight', 'encoder.encoders.races.encoder.encoder_dict.numerical.weight', 'encoder.encoders.races.encoder.encoder_dict.numerical.bias', 'encoder.encoders.races.encoder.encoder_dict.timestamp.weight', 'encoder.encoders.races.encoder.encoder_dict.timestamp.bias', 'encoder.encoders.races.encoder.encoder_dict.embedding.biases', 'encoder.encoders.races.backbone.0.lin1.weight', 'encoder.encoders.races.backbone.0.lin1.bias', 'encoder.encoders.races.backbone.0.lin2.weight', 'encoder.encoders.races.backbone.0.lin2.bias', 'encoder.encoders.races.backbone.0.norm1.weight', 'encoder.encoders.races.backbone.0.norm1.bias', 'encoder.encoders.races.backbone.0.norm2.weight', 'encoder.encoders.races.backbone.0.norm2.bias', 'encoder.encoders.races.backbone.0.shortcut.weight', 'encoder.encoders.races.backbone.0.shortcut.bias', 'encoder.encoders.races.backbone.1.lin1.weight', 'encoder.encoders.races.backbone.1.lin1.bias', 'encoder.encoders.races.backbone.1.lin2.weight', 'encoder.encoders.races.backbone.1.lin2.bias', 'encoder.encoders.races.backbone.1.norm1.weight', 'encoder.encoders.races.backbone.1.norm1.bias', 'encoder.encoders.races.backbone.1.norm2.weight', 'encoder.encoders.races.backbone.1.norm2.bias', 'encoder.encoders.races.backbone.2.lin1.weight', 'encoder.encoders.races.backbone.2.lin1.bias', 'encoder.encoders.races.backbone.2.lin2.weight', 'encoder.encoders.races.backbone.2.lin2.bias', 'encoder.encoders.races.backbone.2.norm1.weight', 'encoder.encoders.races.backbone.2.norm1.bias', 'encoder.encoders.races.backbone.2.norm2.weight', 'encoder.encoders.races.backbone.2.norm2.bias', 'encoder.encoders.races.backbone.3.lin1.weight', 'encoder.encoders.races.backbone.3.lin1.bias', 'encoder.encoders.races.backbone.3.lin2.weight', 'encoder.encoders.races.backbone.3.lin2.bias', 'encoder.encoders.races.backbone.3.norm1.weight', 'encoder.encoders.races.backbone.3.norm1.bias', 'encoder.encoders.races.backbone.3.norm2.weight', 'encoder.encoders.races.backbone.3.norm2.bias', 'encoder.encoders.races.decoder.0.weight', 'encoder.encoders.races.decoder.0.bias', 'encoder.encoders.races.decoder.2.weight', 'encoder.encoders.races.decoder.2.bias', 'encoder.encoders.results.encoder.encoder_dict.numerical.weight', 'encoder.encoders.qualifying.encoder.encoder_dict.numerical.weight', 'encoder.encoders.qualifying.encoder.encoder_dict.numerical.bias', 'encoder.encoders.qualifying.encoder.encoder_dict.timestamp.weight', 'encoder.encoders.qualifying.encoder.encoder_dict.timestamp.bias', 'encoder.encoders.qualifying.backbone.0.lin1.weight', 'encoder.encoders.qualifying.backbone.0.lin1.bias', 'encoder.encoders.qualifying.backbone.0.lin2.weight', 'encoder.encoders.qualifying.backbone.0.lin2.bias', 'encoder.encoders.qualifying.backbone.0.norm1.weight', 'encoder.encoders.qualifying.backbone.0.norm1.bias', 'encoder.encoders.qualifying.backbone.0.norm2.weight', 'encoder.encoders.qualifying.backbone.0.norm2.bias', 'encoder.encoders.qualifying.backbone.0.shortcut.weight', 'encoder.encoders.qualifying.backbone.0.shortcut.bias', 'encoder.encoders.qualifying.backbone.1.lin1.weight', 'encoder.encoders.qualifying.backbone.1.lin1.bias', 'encoder.encoders.qualifying.backbone.1.lin2.weight', 'encoder.encoders.qualifying.backbone.1.lin2.bias', 'encoder.encoders.qualifying.backbone.1.norm1.weight', 'encoder.encoders.qualifying.backbone.1.norm1.bias', 'encoder.encoders.qualifying.backbone.1.norm2.weight', 'encoder.encoders.qualifying.backbone.1.norm2.bias', 'encoder.encoders.qualifying.backbone.2.lin1.weight', 'encoder.encoders.qualifying.backbone.2.lin1.bias', 'encoder.encoders.qualifying.backbone.2.lin2.weight', 'encoder.encoders.qualifying.backbone.2.lin2.bias', 'encoder.encoders.qualifying.backbone.2.norm1.weight', 'encoder.encoders.qualifying.backbone.2.norm1.bias', 'encoder.encoders.qualifying.backbone.2.norm2.weight', 'encoder.encoders.qualifying.backbone.2.norm2.bias', 'encoder.encoders.qualifying.backbone.3.lin1.weight', 'encoder.encoders.qualifying.backbone.3.lin1.bias', 'encoder.encoders.qualifying.backbone.3.lin2.weight', 'encoder.encoders.qualifying.backbone.3.lin2.bias', 'encoder.encoders.qualifying.backbone.3.norm1.weight', 'encoder.encoders.qualifying.backbone.3.norm1.bias', 'encoder.encoders.qualifying.backbone.3.norm2.weight', 'encoder.encoders.qualifying.backbone.3.norm2.bias', 'encoder.encoders.qualifying.decoder.0.weight', 'encoder.encoders.qualifying.decoder.0.bias', 'encoder.encoders.qualifying.decoder.2.weight', 'encoder.encoders.qualifying.decoder.2.bias', 'gnn.layers.0.v_lin.weight', 'gnn.layers.0.v_lin.bias', 'gnn.layers.0.norm.weight', 'gnn.layers.0.norm.bias']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01, Train loss: 13.501905059200253, Val metrics: {'r2': -3.8737648024438336, 'mae': 9.117145528680656, 'rmse': np.float64(10.23487571431109)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 02, Train loss: 10.709976612793811, Val metrics: {'r2': -1.8020707574187274, 'mae': 6.548875175321906, 'rmse': np.float64(7.7605024175174275)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  3.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 03, Train loss: 8.768892973041623, Val metrics: {'r2': -0.9809455419365389, 'mae': 5.447440596333965, 'rmse': np.float64(6.525090776159976)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 04, Train loss: 7.9322524421985685, Val metrics: {'r2': -0.6255501317425958, 'mae': 4.945211573991285, 'rmse': np.float64(5.910857282400424)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 05, Train loss: 7.32131954341803, Val metrics: {'r2': -0.35095667672763775, 'mae': 4.524334662264796, 'rmse': np.float64(5.3885386302398794)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 06, Train loss: 6.774766865055561, Val metrics: {'r2': -0.1418936916212432, 'mae': 4.180910639740581, 'rmse': np.float64(4.954081266830614)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 07, Train loss: 6.29888071579436, Val metrics: {'r2': -0.02055514905131539, 'mae': 3.9458744087295683, 'rmse': np.float64(4.68347847670755)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  3.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 08, Train loss: 5.917357801511338, Val metrics: {'r2': -0.007848938511352044, 'mae': 3.860449213143898, 'rmse': np.float64(4.6542318197792785)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 09, Train loss: 5.683574975872462, Val metrics: {'r2': -0.07978769548713682, 'mae': 3.927258692443888, 'rmse': np.float64(4.817475079452381)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 10, Train loss: 5.579096104112485, Val metrics: {'r2': -0.1694002924710818, 'mae': 4.03594890790696, 'rmse': np.float64(5.013394597493396)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  3.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 11, Train loss: 5.551196614000184, Val metrics: {'r2': -0.193352221528728, 'mae': 4.067492649152268, 'rmse': np.float64(5.064477110058386)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 12, Train loss: 5.527779751842524, Val metrics: {'r2': 0.029129928650970305, 'mae': 3.7468032334913155, 'rmse': np.float64(4.56804997354811)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 13, Train loss: 5.432874870095559, Val metrics: {'r2': 0.20229888980681188, 'mae': 3.426090200662453, 'rmse': np.float64(4.140667788628067)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  3.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 14, Train loss: 5.158427379474566, Val metrics: {'r2': 0.23598954100156555, 'mae': 3.2544849174056116, 'rmse': np.float64(4.052284623064642)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 15, Train loss: 5.036977721175523, Val metrics: {'r2': 0.15486716562026925, 'mae': 3.3557336040234356, 'rmse': np.float64(4.261993441428805)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 16, Train loss: 4.968900846951954, Val metrics: {'r2': 0.28160857436721975, 'mae': 3.1069664290051655, 'rmse': np.float64(3.929441790987855)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  3.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 17, Train loss: 4.847433478435893, Val metrics: {'r2': 0.29689500891938037, 'mae': 2.967132325640661, 'rmse': np.float64(3.8874102912375967)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 18, Train loss: 4.752716973853818, Val metrics: {'r2': 0.3083994431758186, 'mae': 2.9512050699375436, 'rmse': np.float64(3.855475580407936)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 19, Train loss: 4.7096456320801146, Val metrics: {'r2': 0.23241709338920835, 'mae': 3.180087206948178, 'rmse': np.float64(4.061747642189041)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:04<00:00,  3.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 20, Train loss: 4.685452202638651, Val metrics: {'r2': 0.31596055470298745, 'mae': 2.9305273735770085, 'rmse': np.float64(3.8343421412543544)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 21, Train loss: 4.696372593455261, Val metrics: {'r2': 0.21457591926034314, 'mae': 3.176078492829062, 'rmse': np.float64(4.108680736019478)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 22, Train loss: 4.645947177922279, Val metrics: {'r2': 0.37030409158441024, 'mae': 2.764799352287848, 'rmse': np.float64(3.678880875463291)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  3.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 23, Train loss: 4.6416857517375, Val metrics: {'r2': 0.2515469737844137, 'mae': 3.0648975376773846, 'rmse': np.float64(4.010814385823332)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 24, Train loss: 4.597029735046186, Val metrics: {'r2': 0.3743878059393231, 'mae': 2.763497268444869, 'rmse': np.float64(3.666932302029147)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 25, Train loss: 4.592360770991868, Val metrics: {'r2': 0.3725078493087931, 'mae': 2.776788962994246, 'rmse': np.float64(3.672437711511744)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:04<00:00,  3.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 26, Train loss: 4.571557973385433, Val metrics: {'r2': 0.11396191675166956, 'mae': 3.4297143214690187, 'rmse': np.float64(4.363917022148197)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 27, Train loss: 4.542882889625163, Val metrics: {'r2': 0.3103656128817327, 'mae': 2.95324237071122, 'rmse': np.float64(3.8499912624728716)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 28, Train loss: 4.504757937914123, Val metrics: {'r2': 0.37657071306431655, 'mae': 2.8121644745051424, 'rmse': np.float64(3.6605293199665803)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:04<00:00,  3.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 29, Train loss: 4.501961780941184, Val metrics: {'r2': 0.3155604694350539, 'mae': 2.953968603290871, 'rmse': np.float64(3.8354633043435924)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 30, Train loss: 4.45504164097534, Val metrics: {'r2': 0.23918297033715374, 'mae': 3.095673937692432, 'rmse': np.float64(4.043806836393006)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 31, Train loss: 4.490570407257326, Val metrics: {'r2': 0.28442628900338063, 'mae': 3.0499135492002476, 'rmse': np.float64(3.921728082664911)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  3.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 32, Train loss: 4.433002250619691, Val metrics: {'r2': 0.2864600628212617, 'mae': 2.978690866318717, 'rmse': np.float64(3.916151030965729)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 33, Train loss: 4.4520677689112516, Val metrics: {'r2': 0.3534986083803695, 'mae': 2.865463597088077, 'rmse': np.float64(3.727649083676773)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 34, Train loss: 4.419386943874464, Val metrics: {'r2': 0.3448029719957776, 'mae': 2.9155757296937423, 'rmse': np.float64(3.7526343450318915)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  3.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 35, Train loss: 4.41709830273754, Val metrics: {'r2': 0.36671069703384007, 'mae': 2.8593121548374256, 'rmse': np.float64(3.689362811457421)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 36, Train loss: 4.4345143338137785, Val metrics: {'r2': 0.3914236161446828, 'mae': 2.764516680290003, 'rmse': np.float64(3.6166612854787012)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 37, Train loss: 4.4091636235324, Val metrics: {'r2': 0.32516728477149504, 'mae': 2.9289224734844646, 'rmse': np.float64(3.8084508390708383)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:04<00:00,  3.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 38, Train loss: 4.35618047299648, Val metrics: {'r2': 0.2555276578314737, 'mae': 3.117389078911098, 'rmse': np.float64(4.000134309970408)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 39, Train loss: 4.353179249430157, Val metrics: {'r2': 0.2600045986043571, 'mae': 3.0909741202910586, 'rmse': np.float64(3.9880886180367767)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 40, Train loss: 4.314565457674572, Val metrics: {'r2': 0.029820864164587402, 'mae': 3.574340503997777, 'rmse': np.float64(4.5664242206317125)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  3.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 41, Train loss: 4.352361529557702, Val metrics: {'r2': 0.2685885527970111, 'mae': 3.0905327461525527, 'rmse': np.float64(3.9648902120687133)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 42, Train loss: 4.303179187805368, Val metrics: {'r2': 0.26013109703829973, 'mae': 3.110930517282021, 'rmse': np.float64(3.987747731778994)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 43, Train loss: 4.293987343267484, Val metrics: {'r2': 0.3324584425848305, 'mae': 2.9664389210218736, 'rmse': np.float64(3.7878209647724383)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:04<00:00,  3.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 44, Train loss: 4.306149010752313, Val metrics: {'r2': 0.30989218770223337, 'mae': 2.974106690273654, 'rmse': np.float64(3.851312520616149)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 45, Train loss: 4.252359159805623, Val metrics: {'r2': 0.22315949715529082, 'mae': 3.1894574126165236, 'rmse': np.float64(4.086168015000594)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 46, Train loss: 4.261922763687291, Val metrics: {'r2': 0.30131189668021785, 'mae': 2.979145859334177, 'rmse': np.float64(3.8751807480222795)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:04<00:00,  3.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 47, Train loss: 4.252069349210886, Val metrics: {'r2': 0.28313516537721306, 'mae': 3.024876239384184, 'rmse': np.float64(3.9252645134681634)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 48, Train loss: 4.235647269164126, Val metrics: {'r2': 0.2467874463143297, 'mae': 3.150780112502889, 'rmse': np.float64(4.023546867555234)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 49, Train loss: 4.2289441709596485, Val metrics: {'r2': 0.2975366938331908, 'mae': 3.009947935676447, 'rmse': np.float64(3.885635974474506)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  3.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 50, Train loss: 4.207926737003385, Val metrics: {'r2': 0.17358190872273127, 'mae': 3.2999475161075273, 'rmse': np.float64(4.214540169704783)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 51, Train loss: 4.210454885837745, Val metrics: {'r2': 0.3005708484190289, 'mae': 3.0042961843028095, 'rmse': np.float64(3.8772352662648486)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 52, Train loss: 4.204692859897897, Val metrics: {'r2': 0.29009088081525536, 'mae': 3.0404399556164434, 'rmse': np.float64(3.9061747387939003)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  3.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 53, Train loss: 4.211000116434766, Val metrics: {'r2': 0.17582073355477057, 'mae': 3.310170851553291, 'rmse': np.float64(4.20882755499081)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 54, Train loss: 4.163125689135579, Val metrics: {'r2': 0.2913052257840144, 'mae': 3.022937324099646, 'rmse': np.float64(3.9028324279680118)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 55, Train loss: 4.15102468156885, Val metrics: {'r2': 0.2454198836070941, 'mae': 3.129265755817105, 'rmse': np.float64(4.027197867041868)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  3.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 56, Train loss: 4.130341848878593, Val metrics: {'r2': 0.312195500164683, 'mae': 2.956773958416406, 'rmse': np.float64(3.8448800544502526)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 57, Train loss: 4.135857281709828, Val metrics: {'r2': 0.20389098225622682, 'mae': 3.217773482380665, 'rmse': np.float64(4.13653364712508)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 58, Train loss: 4.142699304913636, Val metrics: {'r2': 0.23642696897004212, 'mae': 3.128704347448024, 'rmse': np.float64(4.051124405687005)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  3.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 59, Train loss: 4.109323452258932, Val metrics: {'r2': 0.18814679578156457, 'mae': 3.2515026891717294, 'rmse': np.float64(4.177236307945493)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 60, Train loss: 4.102668542054521, Val metrics: {'r2': 0.1839643090771388, 'mae': 3.262359369589475, 'rmse': np.float64(4.187982580195146)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  3.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 61, Train loss: 4.094501357933955, Val metrics: {'r2': 0.2986256325701956, 'mae': 3.0102170841329166, 'rmse': np.float64(3.88262310494471)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 62, Train loss: 4.076662734868654, Val metrics: {'r2': 0.24721189505539332, 'mae': 3.098772207035888, 'rmse': np.float64(4.022413037499917)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 63, Train loss: 4.061173449070617, Val metrics: {'r2': 0.30114410426288885, 'mae': 2.962575721199224, 'rmse': np.float64(3.87564603926481)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:04<00:00,  3.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 64, Train loss: 4.070337723937112, Val metrics: {'r2': 0.25128154781913836, 'mae': 3.133368641118169, 'rmse': np.float64(4.0115255058777075)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 65, Train loss: 4.03379911485813, Val metrics: {'r2': 0.2574666717311399, 'mae': 3.061357596403133, 'rmse': np.float64(3.9949216428235417)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 66, Train loss: 4.038759190620679, Val metrics: {'r2': 0.23573328032474605, 'mae': 3.1596596837601187, 'rmse': np.float64(4.052964164898995)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:04<00:00,  3.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 67, Train loss: 4.007690199298313, Val metrics: {'r2': 0.2685232691488181, 'mae': 3.0538408988781907, 'rmse': np.float64(3.965067155376816)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 68, Train loss: 4.037519335570822, Val metrics: {'r2': 0.21599564909784252, 'mae': 3.1621324840831053, 'rmse': np.float64(4.104965637904045)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 69, Train loss: 3.9819771282851706, Val metrics: {'r2': 0.24067247288784277, 'mae': 3.133102348317444, 'rmse': np.float64(4.039846481086125)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  3.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 70, Train loss: 3.97925018371711, Val metrics: {'r2': 0.22633649179703363, 'mae': 3.167336956994408, 'rmse': np.float64(4.077803985517338)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 71, Train loss: 3.983094607841628, Val metrics: {'r2': 0.19796809921394265, 'mae': 3.227090327359074, 'rmse': np.float64(4.15189260179527)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 72, Train loss: 3.9668857156550597, Val metrics: {'r2': 0.255881795882856, 'mae': 3.1124123513738393, 'rmse': np.float64(3.999182784846357)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:04<00:00,  3.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 73, Train loss: 3.9756975167552273, Val metrics: {'r2': 0.20121037839444023, 'mae': 3.2358254051240354, 'rmse': np.float64(4.143491921350732)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 74, Train loss: 3.955882467296513, Val metrics: {'r2': 0.23971692664425848, 'mae': 3.1234399877553316, 'rmse': np.float64(4.042387575875125)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 75, Train loss: 3.9277525297766576, Val metrics: {'r2': 0.20939989863050068, 'mae': 3.2196906098860776, 'rmse': np.float64(4.122196806180529)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:04<00:00,  3.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 76, Train loss: 3.9360715807739814, Val metrics: {'r2': 0.2270469012458104, 'mae': 3.166563057405756, 'rmse': np.float64(4.075931352407633)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 77, Train loss: 3.9219800535343037, Val metrics: {'r2': 0.1861919841953309, 'mae': 3.2675706583894564, 'rmse': np.float64(4.182262339974152)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 78, Train loss: 3.9219628114212797, Val metrics: {'r2': 0.16404566996283476, 'mae': 3.3091042757989886, 'rmse': np.float64(4.23878672413179)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:04<00:00,  3.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 79, Train loss: 3.902259678292335, Val metrics: {'r2': 0.20982772968968344, 'mae': 3.197242728120578, 'rmse': np.float64(4.121081297543733)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 80, Train loss: 3.910743394250536, Val metrics: {'r2': 0.2018393873139226, 'mae': 3.2161133170844556, 'rmse': np.float64(4.141860198434569)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 81, Train loss: 3.89698803733535, Val metrics: {'r2': 0.1922032206679397, 'mae': 3.2266544031953526, 'rmse': np.float64(4.16678745816994)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:04<00:00,  3.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 82, Train loss: 3.8829006847080634, Val metrics: {'r2': 0.17774119143661582, 'mae': 3.2808995932679057, 'rmse': np.float64(4.2039211041466835)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 83, Train loss: 3.8478196826570388, Val metrics: {'r2': 0.20520127350938544, 'mae': 3.2291230538087285, 'rmse': np.float64(4.133128148787859)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 84, Train loss: 3.869325607683168, Val metrics: {'r2': 0.20709750943201355, 'mae': 3.196127690246444, 'rmse': np.float64(4.12819478263274)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:04<00:00,  3.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 85, Train loss: 3.876220669631876, Val metrics: {'r2': 0.20386878726399305, 'mae': 3.200651603487227, 'rmse': np.float64(4.1365913086326325)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 86, Train loss: 3.867476110088574, Val metrics: {'r2': 0.20107570000721264, 'mae': 3.2481823288765286, 'rmse': np.float64(4.143841209370119)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 87, Train loss: 3.8493458588107647, Val metrics: {'r2': 0.1946649087319533, 'mae': 3.2236515329293427, 'rmse': np.float64(4.160433658726183)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:04<00:00,  3.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 88, Train loss: 3.8436705578290007, Val metrics: {'r2': 0.1874970613339797, 'mae': 3.247718027423203, 'rmse': np.float64(4.178907516175429)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 89, Train loss: 3.829982096982869, Val metrics: {'r2': 0.20626061344356772, 'mae': 3.2160595943868837, 'rmse': np.float64(4.13037283008181)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 90, Train loss: 3.854880909975401, Val metrics: {'r2': 0.2051152586186309, 'mae': 3.2137735428297285, 'rmse': np.float64(4.133351790910278)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  3.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 91, Train loss: 3.8480285103650886, Val metrics: {'r2': 0.22388058273384204, 'mae': 3.164093670051896, 'rmse': np.float64(4.084271125687469)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 92, Train loss: 3.842098113467801, Val metrics: {'r2': 0.20381751004945636, 'mae': 3.2182028679666157, 'rmse': np.float64(4.136724521262196)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  3.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 93, Train loss: 3.836559377675662, Val metrics: {'r2': 0.1913520253723332, 'mae': 3.2316422199040313, 'rmse': np.float64(4.168982203271982)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  3.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 94, Train loss: 3.8376917932140064, Val metrics: {'r2': 0.2329038641014819, 'mae': 3.1624365920612796, 'rmse': np.float64(4.060459538108542)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 95, Train loss: 3.8281218544614406, Val metrics: {'r2': 0.1986819273862942, 'mae': 3.2335367291628243, 'rmse': np.float64(4.150044547092945)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  3.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 96, Train loss: 3.8391634966564805, Val metrics: {'r2': 0.1853709874783943, 'mae': 3.2641688685140053, 'rmse': np.float64(4.184371411175098)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 97, Train loss: 3.8397910172677308, Val metrics: {'r2': 0.2060044025043426, 'mae': 3.2167937492798706, 'rmse': np.float64(4.13103939730529)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 98, Train loss: 3.8309801700018276, Val metrics: {'r2': 0.18884725584342898, 'mae': 3.2584563622254885, 'rmse': np.float64(4.1754338770447275)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  3.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 99, Train loss: 3.83084807497662, Val metrics: {'r2': 0.20531309772707518, 'mae': 3.2324727295714375, 'rmse': np.float64(4.132837383299486)}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:03<00:00,  4.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 100, Train loss: 3.816460137781194, Val metrics: {'r2': 0.2048383071074874, 'mae': 3.2287177874553024, 'rmse': np.float64(4.134071793616806)}\n",
            "Best Val metrics: {'r2': 0.3712960643930062, 'mae': 2.763410614232819, 'rmse': np.float64(3.675982025317781)}\n",
            "Best test metrics: {'r2': -0.06436639316943205, 'mae': 4.36718601795665, 'rmse': np.float64(5.375457500226308)}\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 900x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = Model(\n",
        "    data=data,\n",
        "    col_stats_dict=col_stats_dict,\n",
        "    num_layers=1,\n",
        "    channels=128,\n",
        "    out_channels=1,\n",
        "    aggr=\"sum\",\n",
        "    norm=\"batch_norm\",\n",
        ").to(device)\n",
        "\n",
        "#copiamo i pesi del modello perché così possiamo vedere se stiamo effettivamente modificando model:\n",
        "initial_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "\n",
        "# Edge types con almeno un arco nel training set: prenderemo solo questi per il \n",
        "# pre training process:\n",
        "valid_edge_types = []\n",
        "for edge_type in data.edge_types:\n",
        "    edge_index = data[edge_type].edge_index\n",
        "    if edge_index.size(1) > 0:\n",
        "        valid_edge_types.append(edge_type)\n",
        "\n",
        "train_data = next(iter(loader_dict[\"train\"]))\n",
        "active_edge_types = [et for et in valid_edge_types if et in train_data.edge_types and train_data[et].edge_index.size(1) > 0]\n",
        "\n",
        "\n",
        "for edge_type in active_edge_types:\n",
        "    print(f\"\\nPretraining per edge_type: {edge_type}\")\n",
        "    #model.load_state_dict(initial_state)\n",
        "    edge_model = EdgePredictionModel(model)\n",
        "    pretrain_optimizer = torch.optim.Adam(edge_model.parameters(), lr=0.001)\n",
        "    try:\n",
        "        pretrain_edge_prediction(edge_model, loader_dict[\"train\"], edge_type=edge_type, optimizer=pretrain_optimizer, epochs=50)\n",
        "        model.load_state_dict(edge_model.model.state_dict())  # salva pesi aggiornati\n",
        "        #break\n",
        "    except ValueError as e:\n",
        "        print(f\"skipping edge_type {edge_type}: {e}\")\n",
        "\n",
        "\n",
        "after_state = model.state_dict()\n",
        "\n",
        "#verifica se sono cambiati\n",
        "changed_params = [k for k in initial_state if not torch.equal(initial_state[k], after_state[k])]\n",
        "print(\"Parametri modificati:\", changed_params if changed_params else \"Nessuno è cambiato\")\n",
        "\n",
        "\n",
        "model.load_state_dict(edge_model.model.state_dict())  #in teoria inutile\n",
        "\n",
        "\n",
        "#down stream\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
        "epochs = 100\n",
        "total_steps = epochs * len(loader_dict[\"train\"])\n",
        "warmup_steps = int(0.1 * total_steps)  # 10% warmup\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
        "state_dict = None\n",
        "best_val_metric = -math.inf if higher_is_better else math.inf\n",
        "\n",
        "#per mantenere la storia dei MAE nel tempo:\n",
        "val_metr_history = []\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss = train(model, optimizer, scheduler)\n",
        "    val_pred = test(model, loader_dict[\"val\"])\n",
        "    #val_metrics = task.evaluate(val_pred, val_table)\n",
        "    val_metrics = custom_evaluate(val_pred, val_table, task.metrics)\n",
        "\n",
        "    val_metr_history.append(val_metrics[tune_metric])\n",
        "\n",
        "    print(f\"Epoch: {epoch:02d}, Train loss: {train_loss}, Val metrics: {val_metrics}\")\n",
        "\n",
        "    if (higher_is_better and val_metrics[tune_metric] > best_val_metric) or (\n",
        "            not higher_is_better and val_metrics[tune_metric] < best_val_metric\n",
        "    ):\n",
        "        best_val_metric = val_metrics[tune_metric]\n",
        "        state_dict = copy.deepcopy(model.state_dict())\n",
        "\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "val_pred = test(model, loader_dict[\"val\"])\n",
        "val_metrics = custom_evaluate(val_pred, val_table, task.metrics)\n",
        "print(f\"Best Val metrics: {val_metrics}\")\n",
        "\n",
        "test_table = task.get_table(\"test\", mask_input_cols=False)\n",
        "test_pred = test(model,loader_dict[\"test\"])\n",
        "test_metrics = custom_evaluate(test_pred, test_table, task.metrics)\n",
        "print(f\"Best test metrics: {test_metrics}\")\n",
        "\n",
        "plot_validation_metrics([val_metr_history], [\"basic model\"],  metric_name=tune_metric)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkY-BYdoanwe"
      },
      "source": [
        "# Import a predefined model to use it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMAAXTPFanwe"
      },
      "outputs": [],
      "source": [
        "# model.load_state_dict(torch.load('best_model_GAT_head2.pth', map_location=torch.device('cpu')))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
