{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andrea-1704/Pytorch_Geometric_tutorial/blob/main/train_model_baseline_f1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiCB_SVeanwO"
      },
      "source": [
        "In questo netebook proviamo ad implementare un processo di **pre gtraining** basato su self supervision.\n",
        "Il modello dovrebbe riconoscere se un nodo fa parte del grafo reale o di una versione corrotta (negativa).\n",
        "Quindi:\n",
        "\n",
        "    Se un nodo ha embedding simile al global summary del grafo reale → POSITIVO\n",
        "\n",
        "    Se un nodo viene da un grafo \"corrotto\" (shufflato) → NEGATIVO\n",
        "\n",
        "Fondamentalmente l'idea è quella di, prendere **x** come input delle features originali. Costruire **z=GNN(x, edge_index)**, ovvero le rappresentazioni dei nodi che esistono realmente nel grafo. Calcolare **summary=READOUT(z)** ovvero una sorta di rappresentazione globale del modello che descriva le informazioni generali topologiche per il grafo.\n",
        "\n",
        "A questo punto andremo a perturbare x, calcolando **x_corrupted=x corrotto** che in genere viene calcolato tramite una perturbazione, o shuffle delle node features: semplicemente cambiamo l'ordine delle features dei nodi. Calcoliamo come prima **z_corrupted=GNN(x_corrupted, edge_index)**.\n",
        "\n",
        "Poi calcoliamo questi valori di score: **logits = discriminator(z, summary)**, **logits=discriminator(z_corrupted, summary)**, dove l'obiettivo è quello di otteenere valori vicini ad uno per il primo dei due e vicini a zero per il secondo.\n",
        "\n",
        "L'obiettivo adesso sarà quello di compiere un processo di classificazione binaria finalizzata a riconoscere come positivi i sampel che derivano dalla struttura reale (z) e come negativi quelli che invece provengono dalla struttura perturbata (z_corrupted)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "zNziUzq9nTdU"
      },
      "outputs": [],
      "source": [
        "# !pip install torch-scatter -f https://data.pyg.org/whl/torch-2.4.0+cpu.html\n",
        "# !pip install torch-sparse -f https://data.pyg.org/whl/torch-2.4.0+cpu.html\n",
        "# !pip install torch-cluster -f https://data.pyg.org/whl/torch-2.4.0+cpu.html\n",
        "# !pip install torch-spline-conv -f https://data.pyg.org/whl/torch-2.4.0+cpu.html\n",
        "# !pip install torch-geometric==2.6.0 -f https://data.pyg.org/whl/torch-2.4.0+cpu.html\n",
        "# !pip install pyg-lib -f https://data.pyg.org/whl/torch-2.4.0+cpu.html\n",
        "\n",
        "# !pip install pytorch_frame[full]==1.2.2\n",
        "# !pip install relbench[full]==1.0.0\n",
        "# !pip uninstall -y pyg_lib torch  # Uninstall current versions\n",
        "# !pip install torch==2.6.0  # Reinstall your desired PyTorch version\n",
        "# !pip install --no-cache-dir git+https://github.com/pyg-team/pyg-lib.git # Install pyg-lib; --no-cache-dir ensures a fresh install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8VinWw-anwR"
      },
      "source": [
        "New libraries to run on colab:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "cTB7Q6qQanwR"
      },
      "outputs": [],
      "source": [
        "# !pip install torch==2.6.0+cu118 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# !pip install pyg-lib -f https://data.pyg.org/whl/torch-2.6.0+cu118.html\n",
        "# !pip install torch-scatter -f https://data.pyg.org/whl/torch-2.6.0+cu118.html\n",
        "# !pip install torch-sparse -f https://data.pyg.org/whl/torch-2.6.0+cu118.html\n",
        "# !pip install torch-cluster -f https://data.pyg.org/whl/torch-2.6.0+cu118.html\n",
        "# !pip install torch-spline-conv -f https://data.pyg.org/whl/torch-2.6.0+cu118.html\n",
        "# !pip install torch-geometric==2.6.0 -f https://data.pyg.org/whl/torch-2.6.0+cu118.html\n",
        "\n",
        "# !pip install pytorch_frame[full]==1.2.2\n",
        "# !pip install relbench[full]==1.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "F454ta1Zg0Oq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import relbench\n",
        "import numpy as np\n",
        "from torch.nn import BCEWithLogitsLoss, L1Loss\n",
        "from relbench.datasets import get_dataset\n",
        "from relbench.tasks import get_task\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import torch_geometric\n",
        "import torch_frame\n",
        "from torch_geometric.seed import seed_everything\n",
        "from relbench.modeling.utils import get_stype_proposal\n",
        "from collections import defaultdict\n",
        "import requests\n",
        "from io import StringIO\n",
        "from torch_frame.config.text_embedder import TextEmbedderConfig\n",
        "from relbench.modeling.graph import make_pkey_fkey_graph\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "import copy\n",
        "from typing import Any, Dict, List\n",
        "from torch import Tensor\n",
        "from torch.nn import Embedding, ModuleDict\n",
        "from torch_frame.data.stats import StatType\n",
        "from torch_geometric.data import HeteroData\n",
        "from torch_geometric.nn import MLP\n",
        "from torch_geometric.typing import NodeType\n",
        "from relbench.modeling.nn import HeteroEncoder, HeteroGraphSAGE, HeteroTemporalEncoder\n",
        "from relbench.modeling.graph import get_node_train_table_input, make_pkey_fkey_graph\n",
        "from torch_geometric.loader import NeighborLoader\n",
        "import pyg_lib\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#per lo scheduler\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch_geometric.nn import Linear\n",
        "from torch_geometric.utils import softmax\n",
        "from torch_geometric.utils import degree\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTytIKXvanwT"
      },
      "source": [
        "# Dataset and task creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DWB-Kf6nl2y",
        "outputId": "27370877-4a01-4366-85a6-2dfe4835d23b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "dataset = get_dataset(\"rel-f1\", download=True)\n",
        "task = get_task(\"rel-f1\", \"driver-position\", download=True)\n",
        "\n",
        "train_table = task.get_table(\"train\")\n",
        "val_table = task.get_table(\"val\")\n",
        "test_table = task.get_table(\"test\")\n",
        "\n",
        "out_channels = 1\n",
        "# one because we are estimating one single value.\n",
        "loss_fn = L1Loss()\n",
        "# this is the mae loss and is used when have regressions tasks.\n",
        "tune_metric = \"mae\"\n",
        "higher_is_better = False\n",
        "\n",
        "seed_everything(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "root_dir = \"./data\"\n",
        "\n",
        "db = dataset.get_db()\n",
        "col_to_stype_dict = get_stype_proposal(db)\n",
        "#this is used to get the stype of the columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_validation_metrics(metric_histories, model_names=None, metric_name=\"MAE\", informationsTitle=\"\"):\n",
        "    \"\"\"\n",
        "    Plotta l'andamento del metric_name per più modelli nel tempo.\n",
        "\n",
        "    Args:\n",
        "        metric_histories (list of lists): Lista di liste, ognuna rappresenta i valori di metriche per un modello.\n",
        "        model_names (list of str): Nomi dei modelli (opzionale).\n",
        "        metric_name (str): Nome della metrica da visualizzare.\n",
        "        informationsTitle (str): info aggiungitive da mettere nel titolo (conf generale dei parametri ecc).\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(9, 5))\n",
        "\n",
        "    if model_names is None:\n",
        "        model_names = [f\"Model {i+1}\" for i in range(len(metric_histories))]\n",
        "\n",
        "    for metrics, name in zip(metric_histories, model_names):\n",
        "        plt.plot(metrics, marker='o', label=f'{name} {metric_name}')\n",
        "\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(metric_name)\n",
        "    plt.title(f\"{metric_name} over Epochs for Multiple Models {informationsTitle}\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def custom_evaluate(pred: np.ndarray, target_table, metrics) -> dict:\n",
        "    \"\"\"Custom evaluation function to replace task.evaluate.\"\"\"\n",
        "\n",
        "    # Extract target values from the target table\n",
        "    target = target_table.df[task.target_col].to_numpy()\n",
        "\n",
        "    # Check for length mismatch\n",
        "    if len(pred) != len(target):\n",
        "        raise ValueError(\n",
        "            f\"The length of pred and target must be the same (got \"\n",
        "            f\"{len(pred)} and {len(target)}, respectively).\"\n",
        "        )\n",
        "\n",
        "    # Calculate metrics\n",
        "    results = {}\n",
        "    for metric_fn in metrics:\n",
        "        if metric_fn.__name__ == \"rmse\":  # Handle RMSE specifically\n",
        "            results[\"rmse\"] = np.sqrt(np.mean((target - pred)**2))\n",
        "        else:  # Handle other metrics (if any)\n",
        "            results[metric_fn.__name__] = metric_fn(target, pred)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rmse(true, pred):\n",
        "    \"\"\"Calculate the Root Mean Squared Error (RMSE).\"\"\"\n",
        "    return np.sqrt(np.mean((true - pred)**2)) # Calculate RMSE manually"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVsgNYpuanwU"
      },
      "source": [
        "# Embedder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "QQHYmgIxkX1j"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# from typing import List, Optional\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "# from torch import Tensor\n",
        "\n",
        "\n",
        "# class GloveTextEmbedding:\n",
        "#     def __init__(self, device: Optional[torch.device\n",
        "#                                        ] = None):\n",
        "#         self.model = SentenceTransformer(\n",
        "#             \"sentence-transformers/average_word_embeddings_glove.6B.300d\",\n",
        "#             device=device,\n",
        "#         )\n",
        "\n",
        "#     def __call__(self, sentences: List[str]) -> Tensor:\n",
        "#         return torch.from_numpy(self.model.encode(sentences))\n",
        "\n",
        "\n",
        "class LightweightGloveEmbedder:\n",
        "    def __init__(self, device=None):\n",
        "        self.device = device\n",
        "        self.embeddings = defaultdict(lambda: np.zeros(300))\n",
        "        self._load_embeddings()\n",
        "\n",
        "    def _load_embeddings(self):\n",
        "        try:\n",
        "            #(senza bisogno di estrarre zip\n",
        "            url = \"https://huggingface.co/stanfordnlp/glove/resolve/main/glove.6B.300d.txt\"\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            for line in StringIO(response.text):\n",
        "                parts = line.split()\n",
        "                word = parts[0]\n",
        "                vector = np.array(parts[1:], dtype=np.float32)\n",
        "                self.embeddings[word] = vector\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Couldn't load GloVe embeddings ({str(e)}). Using zero vectors.\")\n",
        "\n",
        "    def __call__(self, sentences):\n",
        "        results = []\n",
        "        for text in sentences:\n",
        "            words = text.lower().split()\n",
        "            vectors = [self.embeddings[w] for w in words if w in self.embeddings]\n",
        "            if vectors:\n",
        "                avg_vector = np.mean(vectors, axis=0)\n",
        "            else:\n",
        "                avg_vector = np.zeros(300)\n",
        "            results.append(avg_vector)\n",
        "\n",
        "        tensor = torch.tensor(np.array(results), dtype=torch.float32)\n",
        "        return tensor.to(self.device) if self.device else tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-BBpUrakdwY",
        "outputId": "4a580142-6aa6-41bf-fd19-e986f542f3bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Couldn't load GloVe embeddings (404 Client Error: Not Found for url: https://huggingface.co/stanfordnlp/glove/resolve/main/glove.6B.300d.txt). Using zero vectors.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_frame/utils/io.py:113: UserWarning: Weights only load failed. Please file an issue to make `torch.load(weights_only=True)` compatible in your case. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global.\n",
            "  warnings.warn(f\"{warn_msg} Please use \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_frame/utils/io.py:113: UserWarning: Weights only load failed. Please file an issue to make `torch.load(weights_only=True)` compatible in your case. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global.\n",
            "  warnings.warn(f\"{warn_msg} Please use \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_frame/utils/io.py:113: UserWarning: Weights only load failed. Please file an issue to make `torch.load(weights_only=True)` compatible in your case. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global.\n",
            "  warnings.warn(f\"{warn_msg} Please use \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_frame/utils/io.py:113: UserWarning: Weights only load failed. Please file an issue to make `torch.load(weights_only=True)` compatible in your case. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global.\n",
            "  warnings.warn(f\"{warn_msg} Please use \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_frame/utils/io.py:113: UserWarning: Weights only load failed. Please file an issue to make `torch.load(weights_only=True)` compatible in your case. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global.\n",
            "  warnings.warn(f\"{warn_msg} Please use \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_frame/utils/io.py:113: UserWarning: Weights only load failed. Please file an issue to make `torch.load(weights_only=True)` compatible in your case. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global.\n",
            "  warnings.warn(f\"{warn_msg} Please use \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_frame/utils/io.py:113: UserWarning: Weights only load failed. Please file an issue to make `torch.load(weights_only=True)` compatible in your case. Please use `torch.serialization.add_safe_globals([scalar])` to allowlist this global.\n",
            "  warnings.warn(f\"{warn_msg} Please use \"\n"
          ]
        }
      ],
      "source": [
        "text_embedder_cfg = TextEmbedderConfig(\n",
        "    text_embedder=LightweightGloveEmbedder(device=device), batch_size=256\n",
        ")\n",
        "\n",
        "data, col_stats_dict = make_pkey_fkey_graph(\n",
        "    #Solution if not working: !pip install --upgrade torch torchvision transformers\n",
        "    db,\n",
        "    col_to_stype_dict=col_to_stype_dict,  # speficied column types\n",
        "    text_embedder_cfg=text_embedder_cfg,  # our chosen text encoder\n",
        "    cache_dir=os.path.join(\n",
        "        root_dir, f\"rel-f1_materialized_cache\"\n",
        "    ),  # store materialized graph for convenience\n",
        ")# create a graph how relbench requires."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "HUHVG-g6lM-b"
      },
      "outputs": [],
      "source": [
        "loader_dict = {}\n",
        "\n",
        "for split, table in [\n",
        "    (\"train\", train_table),\n",
        "    (\"val\", val_table),\n",
        "    (\"test\", test_table),\n",
        "]:\n",
        "    table_input = get_node_train_table_input(\n",
        "        table=table,\n",
        "        task=task,\n",
        "    )#notice that table_input is an object with three elements: nodes, time and transform.\n",
        "    #nodes contains the input nodes\n",
        "    #time contains the time for each node\n",
        "    #transform is the tranformation to be applied to nodes\n",
        "    entity_table = table_input.nodes[0]\n",
        "    #we need to populate the loader_dict with three elements: \"train\", \"val\", and \"test\".\n",
        "    loader_dict[split] = NeighborLoader(\n",
        "        data,\n",
        "        num_neighbors=[\n",
        "            128 for i in range(2)\n",
        "        ],  # we sample subgraphs of depth 2, 128 neighbors per node.\n",
        "        time_attr=\"time\",\n",
        "        input_nodes=table_input.nodes,\n",
        "        input_time=table_input.time,\n",
        "        transform=table_input.transform,\n",
        "        batch_size=512,\n",
        "        temporal_strategy=\"uniform\",\n",
        "        shuffle=split == \"train\",\n",
        "        num_workers=0,\n",
        "        persistent_workers=False,\n",
        "    )#this is the loader for grapg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNecfPt3anwW"
      },
      "source": [
        "Nota che ogni oggetto \"batch\" rappresenta un dizionario in cui le chiavi rappresentano i tipi di nodi oppure i tipi di archi e contengono gli embedding del nodo opppure l'edge index dell'arco (vale a dire un tensore di dimensione 2xnumero di edge, ovvero la lista dei nodi di tipo sorgente che presdentano una relazione di quel tipo con i nodi di tipo destinazione, ovvero quelli della seconda lista)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7DiIz69anwW"
      },
      "source": [
        "# Pre-training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDHxR9BV5fW0"
      },
      "source": [
        "Disciminator, calcola semplicemente il prodotto scalare tra z e summary (embedding globale del grafo di partenza) e predice se è vero o falso.\n",
        "\n",
        "L'obiettivo di questo pre training è quello di costruire un **sunmmary** dell'intero grafo, che in questo caso è calcolato come media degli embedding dei suoi nodi. Successivamente cerchiamo di massimizzare la somiglianza (similarity) tra gli embeddings dei nodi effettivamente presenti dentro il grafo (detti **nodi positivi**) e minimizzare quella con nodi \"corrotti\", ovvero nodi modificati appositamente (detti **nodi negativi**). Speriamo in questo modo che i parametri del modello iniziale (prima di cominciare il training vero e proprio) contengano già informazioni rilevanti sul grafo, ovvero siano in grado di riconoscere una certa struttura degli embeddings dei nodi.\n",
        "\n",
        "In questo modo quindi speriamo che i pesi del modello iniziale non siano \"casuali\" ma contengono già una certa sematica sui nodi, ovvero che gli embeddings dei nodi contengano già rilevanti informazioni sulla struttura del grafo, la \"logica\" dei nodi e le relazioni rilevanti tra i nodi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Classe del discrinatore (non viene usata poi per il down stream task, ma migliorare lei porta a migliorare l'encoder della gnn)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vF4LrxL55fW1"
      },
      "outputs": [],
      "source": [
        "class DGIHead(torch.nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.weight = torch.nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
        "        torch.nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "    def forward(self, x_dict, corrupted_x_dict, summary):\n",
        "        loss = 0\n",
        "        for node_type in x_dict:\n",
        "            z = x_dict[node_type]                     # [N, hidden]\n",
        "            #qui prendiamo la struttura [num_nodes, features] positivi\n",
        "            z_corrupt = corrupted_x_dict[node_type]   # [N, hidden]\n",
        "            #struttura [num_nodes, features] negativi.\n",
        "            if z.shape[0] == 0:\n",
        "              continue\n",
        "            #print(\"x corrotto: \", z_corrupt)\n",
        "\n",
        "            pos = torch.matmul(z, self.weight)        # [N, hidden]\n",
        "            neg = torch.matmul(z_corrupt, self.weight)# [N, hidden]\n",
        "\n",
        "            summary_proj = summary.t()                # [hidden, 1]\n",
        "            #andiamo a trasporre la summary del grafo.\n",
        "\n",
        "            pos_score = torch.matmul(pos, summary_proj).squeeze()  # [N]\n",
        "            #usiamo come metrica di similarity il prodotto.\n",
        "            neg_score = torch.matmul(neg, summary_proj).squeeze()  # [N]\n",
        "\n",
        "            #print(\"il positive score è: \", pos_score)-> qui è già nan!\n",
        "\n",
        "            pos_loss = -torch.log(torch.sigmoid(pos_score) + 1e-15).mean()\n",
        "            #print(\"il positive score dopo la log è: \", pos_loss)\n",
        "\n",
        "            neg_loss = -torch.log(1 - torch.sigmoid(neg_score) + 1e-15).mean()\n",
        "\n",
        "            loss += pos_loss + neg_loss\n",
        "\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Per capire la funzione di forward ricorda che x_dict è il dizionario originale quello, ovvero, che contiene gli emebeddings effettivi per i tipi di nodi. corrupted_x_dict è un dizionario che segue la stessa struttura ma che contiene gli embeddings negativi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Qui sotto definiamo una funzione che calcoli una sintesi dell'intero grafo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def compute_summary(x_dict):\n",
        "    summary_dict = {}\n",
        "    for k, z in x_dict.items():\n",
        "        #se nonm abbiamo nodi di quel tipo escludi\n",
        "        if z.numel() == 0:\n",
        "            summary_dict[k] = torch.zeros(z.shape[1], device=z.device)\n",
        "        else:\n",
        "            summary_dict[k] = torch.tanh(z.mean(dim=0))\n",
        "    return summary_dict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Di seguito si mostra l'implementazione di una funzione che prende come parametro un HeteroGraph e restituisce un altro HeteroGrapf in cui i nodi sono stati corrotti, ovvero mescoliamo in modo randomico le features dei nodi: in pratica un tipo di nodo x riceve casualmente le features del nodo y, per esempoio alla tabella delle corse sono dati deglki embeddings della tabella di costruzioni (chiaramente archi rimangono invariati).\n",
        "\n",
        "Lo vorremmo fare direttamente per le x, ovvero le features dei tipi di nodi. Tuttavia questo nel batch non è direttamente presente, ma è presente n_id che associa, ordinatamente, ad ogni nodo un id del nodo aa cui fa riferiemnto e quindi successivamente da quell'id si recuperano i suoi embeddings. Possiamo quindi modificare semplicemente l'ordine di n_id per cambiare i nodi di partenza."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def corrupt_features(batch: HeteroData) -> HeteroData:\n",
        "    corrupted = copy.deepcopy(batch)\n",
        "    #costruisco una clone del batch di grafo per evitare di modificare\n",
        "    #il grafo originale\n",
        "    for node_type in corrupted.node_types:\n",
        "        #print(corrupted[node_type])\n",
        "        #if hasattr(corrupted[node_type], \"n_id\"):\n",
        "        # Use embeddings, permute n_ids\n",
        "        n_id = corrupted[node_type].n_id\n",
        "        perm = torch.randperm(n_id.size(0))\n",
        "        corrupted[node_type].n_id = n_id[perm]\n",
        "    return corrupted\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Funzione che effettua il pre training per qualche epoca."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkGCazfH5fW2"
      },
      "outputs": [],
      "source": [
        "def train_dgi(model, discriminator, optimizer, loader, device, entity_table, epochs=20):\n",
        "    model.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        for batch in pbar:\n",
        "            batch = batch.to(device)\n",
        "\n",
        "            # Corrupt the batch\n",
        "            corrupted_batch = corrupt_features(batch)\n",
        "            corrupted_batch = corrupted_batch.to(device)\n",
        "\n",
        "            # Get positive and negative embeddings + summary\n",
        "            z_dict, summary = model.pretrain_dgi_forward(batch, entity_table=entity_table)\n",
        "            corrupted_z_dict, _ = model.pretrain_dgi_forward(corrupted_batch, entity_table=entity_table)\n",
        "\n",
        "            # Compute DGI loss\n",
        "            loss = discriminator(z_dict, corrupted_z_dict, summary)\n",
        "\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            pbar.set_postfix({\"DGI Loss\": loss.item()})\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Average DGI Loss: {epoch_loss / len(loader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3AfboisanwY"
      },
      "source": [
        "## Graphormer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "PoAmtoPsanwZ"
      },
      "outputs": [],
      "source": [
        "_spatial_bias_cache = None\n",
        "_node_offset_cache = None\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def compute_spatial_bias(edge_index_dict, x_dict):\n",
        "    global _spatial_bias_cache, _node_offset_cache\n",
        "    if _spatial_bias_cache is not None:\n",
        "        return _spatial_bias_cache, _node_offset_cache\n",
        "    #creiamo un grafo diretto con Networkx\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    node_offset = {}\n",
        "    curr_offset = 0\n",
        "\n",
        "    #aggiungiamo i nodi con offset per mantenere indici globali univoci\n",
        "    for node_type, x in x_dict.items():\n",
        "        node_offset[node_type] = curr_offset\n",
        "        for i in range(x.size(0)):\n",
        "            G.add_node(curr_offset + i, type=node_type)\n",
        "        curr_offset += x.size(0)\n",
        "\n",
        "    #Aggiungiamo gli archi con offset\n",
        "    for (src_type, _, dst_type), edge_index in edge_index_dict.items():\n",
        "        src_offset = node_offset[src_type]\n",
        "        dst_offset = node_offset[dst_type]\n",
        "        src, dst = edge_index\n",
        "        for s, d in zip(src.tolist(), dst.tolist()):\n",
        "            G.add_edge(src_offset + s, dst_offset + d)\n",
        "\n",
        "\n",
        "    spatial_bias = defaultdict(lambda: -1)\n",
        "\n",
        "\n",
        "\n",
        "    for node in G.nodes():\n",
        "        lengths = nx.single_source_dijkstra_path_length(G, node)\n",
        "        for target, dist in lengths.items():\n",
        "            spatial_bias[(node, target)] = dist\n",
        "        #quelli non raggiungibili li lasciamo con default value, ovvero -1\n",
        "\n",
        "    _spatial_bias_cache = spatial_bias\n",
        "    _node_offset_cache = node_offset\n",
        "\n",
        "    return spatial_bias, node_offset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "rN-8kKQianwa"
      },
      "outputs": [],
      "source": [
        "class HeteroGraphormerLayerComplete(nn.Module):\n",
        "    def __init__(self, channels, edge_types, device, num_heads=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.num_heads = num_heads\n",
        "        self.channels = channels\n",
        "        self.head_dim = channels // num_heads\n",
        "\n",
        "        assert self.channels % num_heads == 0, \"channels must be divisible by num_heads\"\n",
        "\n",
        "        self.q_lin = Linear(channels, channels)\n",
        "        self.k_lin = Linear(channels, channels)\n",
        "        self.v_lin = Linear(channels, channels)\n",
        "        self.out_lin = Linear(channels, channels)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm = nn.LayerNorm(channels)\n",
        "\n",
        "        # Registriamo i bias per ogni tipo di edge nel __init__\n",
        "        self.edge_type_bias = nn.ParameterDict({\n",
        "            \"__\".join(edge_type): nn.Parameter(torch.randn(1))\n",
        "            for edge_type in edge_types\n",
        "        })\n",
        "\n",
        "    def compute_total_degrees(self, x_dict, edge_index_dict):\n",
        "        device = self.device\n",
        "        in_deg = defaultdict(lambda: torch.zeros(0, device=device))\n",
        "        out_deg = defaultdict(lambda: torch.zeros(0, device=device))\n",
        "        for edge_type, edge_index in edge_index_dict.items():\n",
        "            src_type, _, dst_type = edge_type\n",
        "            src = edge_index[0]\n",
        "            dst = edge_index[1]\n",
        "\n",
        "            num_src = x_dict[src_type].size(0)\n",
        "            num_dst = x_dict[dst_type].size(0)\n",
        "\n",
        "            if out_deg[src_type].numel() == 0:\n",
        "                out_deg[src_type] = torch.zeros(num_src, device=device)\n",
        "            if in_deg[dst_type].numel() == 0:\n",
        "                in_deg[dst_type] = torch.zeros(num_dst, device=device)\n",
        "\n",
        "            out_deg[src_type] += degree(src, num_nodes=num_src)\n",
        "            in_deg[dst_type]  += degree(dst, num_nodes=num_dst)\n",
        "\n",
        "        total_deg = {\n",
        "            node_type: in_deg[node_type] + out_deg[node_type]\n",
        "            for node_type in x_dict\n",
        "        }\n",
        "\n",
        "        return total_deg\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict):\n",
        "        #print(edge_index_dict)\n",
        "        self.spatial_bias, self.node_offset = compute_spatial_bias(edge_index_dict, x_dict)\n",
        "\n",
        "        out_dict = {k: torch.zeros_like(v) for k, v in x_dict.items()}\n",
        "        for edge_type, edge_index in edge_index_dict.items():\n",
        "            src_type, _, dst_type = edge_type\n",
        "            x_src, x_dst = x_dict[src_type], x_dict[dst_type]\n",
        "\n",
        "            #src, dst = edge_index\n",
        "            src = edge_index[0]\n",
        "            dst = edge_index[1]\n",
        "\n",
        "            Q = self.q_lin(x_dst).view(-1, self.num_heads, self.head_dim)\n",
        "            K = self.k_lin(x_src).view(-1, self.num_heads, self.head_dim)\n",
        "            V = self.v_lin(x_src).view(-1, self.num_heads, self.head_dim)\n",
        "\n",
        "            attn_scores = (Q[dst] * K[src]).sum(dim=-1) / self.head_dim**0.5\n",
        "            src_offset = self.node_offset[src_type]\n",
        "            dst_offset = self.node_offset[dst_type]\n",
        "\n",
        "            spatial_bias_vals = []\n",
        "            for s, d in zip(src.tolist(), dst.tolist()):\n",
        "                global_s = src_offset + s\n",
        "                global_d = dst_offset + d\n",
        "                dist = self.spatial_bias.get((global_d, global_s), -1.0)\n",
        "                spatial_bias_vals.append(dist)\n",
        "\n",
        "            spatial_bias_tensor = torch.tensor(spatial_bias_vals, dtype=torch.float, device=self.device)\n",
        "            attn_scores = attn_scores + spatial_bias_tensor.unsqueeze(-1)  # broadcast su heads\n",
        "\n",
        "\n",
        "            bias_name = \"__\".join(edge_type)\n",
        "            attn_scores = attn_scores + self.edge_type_bias[bias_name]\n",
        "\n",
        "            attn_weights = softmax(attn_scores, dst)\n",
        "            attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "            out = V[src] * attn_weights.unsqueeze(-1)\n",
        "            out = out.view(-1, self.channels)\n",
        "\n",
        "            out_dict[dst_type].index_add_(0, dst, out)\n",
        "\n",
        "        #calcolo della degree centrality\n",
        "\n",
        "        total_deg = self.compute_total_degrees(x_dict, edge_index_dict)\n",
        "\n",
        "\n",
        "        for node_type in out_dict:\n",
        "\n",
        "            degree_embed = total_deg[node_type].view(-1, 1)                                                                                  # Assicurati che sia una colonna\n",
        "            degree_embed = degree_embed.expand(-1, self.channels)                                                                            # Espandi lungo la dimensione dei canali\n",
        "\n",
        "\n",
        "            out_dict[node_type] = out_dict[node_type] + degree_embed\n",
        "\n",
        "\n",
        "        for node_type in out_dict:\n",
        "            out_dict[node_type] = self.norm(out_dict[node_type] + x_dict[node_type])\n",
        "\n",
        "        return out_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "xz8Y3N_kanwa"
      },
      "outputs": [],
      "source": [
        "class HeteroGraphormer(torch.nn.Module):\n",
        "    def __init__(self, node_types, edge_types, channels, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.layers = torch.nn.ModuleList([\n",
        "            HeteroGraphormerLayerComplete(channels, edge_types, device) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict, *args, **kwargs):\n",
        "        for layer in self.layers:\n",
        "            x_dict = layer(x_dict, edge_index_dict)\n",
        "        return x_dict\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for layer in self.layers:\n",
        "            if hasattr(layer, \"reset_parameters\"):\n",
        "                layer.reset_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3m3jEqClQnw"
      },
      "outputs": [],
      "source": [
        "class Model(torch.nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        data: HeteroData, #notice that \"data2 is the graph we created with function make_pkey_fkey_graph\n",
        "        col_stats_dict: Dict[str, Dict[str, Dict[StatType, Any]]],\n",
        "        num_layers: int,\n",
        "        channels: int,\n",
        "        out_channels: int,\n",
        "        aggr: str,\n",
        "        norm: str,\n",
        "        # List of node types to add shallow embeddings to input\n",
        "        shallow_list: List[NodeType] = [],\n",
        "        # ID awareness\n",
        "        id_awareness: bool = False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = HeteroEncoder(\n",
        "            channels=channels,\n",
        "            node_to_col_names_dict={\n",
        "                node_type: data[node_type].tf.col_names_dict\n",
        "                for node_type in data.node_types\n",
        "            },\n",
        "            node_to_col_stats=col_stats_dict,\n",
        "        )\n",
        "        self.temporal_encoder = HeteroTemporalEncoder(\n",
        "            node_types=[\n",
        "                node_type for node_type in data.node_types if \"time\" in data[node_type]\n",
        "            ],\n",
        "            channels=channels,\n",
        "        )\n",
        "        self.gnn = HeteroGraphormer(\n",
        "            node_types=data.node_types,\n",
        "            edge_types=data.edge_types,\n",
        "            channels=channels,\n",
        "            num_layers=num_layers,\n",
        "        )\n",
        "        self.head = MLP(\n",
        "            channels,#one, since we are doing regression\n",
        "            out_channels=out_channels,\n",
        "            norm=norm,\n",
        "            num_layers=1,\n",
        "        )\n",
        "        self.embedding_dict = ModuleDict(\n",
        "            {\n",
        "                node: Embedding(data.num_nodes_dict[node], channels)\n",
        "                for node in shallow_list\n",
        "            }\n",
        "        )\n",
        "\n",
        "        self.id_awareness_emb = None\n",
        "        if id_awareness:\n",
        "            self.id_awareness_emb = torch.nn.Embedding(1, channels)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.encoder.reset_parameters()\n",
        "        self.temporal_encoder.reset_parameters()\n",
        "        self.gnn.reset_parameters()\n",
        "        self.head.reset_parameters()\n",
        "        for embedding in self.embedding_dict.values():\n",
        "            torch.nn.init.normal_(embedding.weight, std=0.1)\n",
        "        if self.id_awareness_emb is not None:\n",
        "            self.id_awareness_emb.reset_parameters()\n",
        "            \n",
        "\n",
        "\n",
        "    def pretrain_dgi_forward(self, batch: HeteroData, entity_table: NodeType):\n",
        "        x_dict = self.encoder(batch.tf_dict)\n",
        "        # Costruiamo x_dict\n",
        "\n",
        "        if self.id_awareness_emb is not None:\n",
        "            for node_type in x_dict:\n",
        "                x_dict[node_type] += self.id_awareness_emb.weight\n",
        "\n",
        "        if hasattr(batch[entity_table], \"seed_time\"):\n",
        "            rel_time_dict = self.temporal_encoder(\n",
        "                batch[entity_table].seed_time,\n",
        "                batch.time_dict,\n",
        "                batch.batch_dict\n",
        "            )\n",
        "            for node_type, rel_time in rel_time_dict.items():\n",
        "                x_dict[node_type] += rel_time\n",
        "        # Aggiungiamo il temporal encoding se presente\n",
        "\n",
        "        for node_type, embedding in self.embedding_dict.items():\n",
        "            x_dict[node_type] += embedding(batch[node_type].n_id)\n",
        "        # Aggiungiamo gli shallow embeddings per i nodi che lo hanno abilitato\n",
        "\n",
        "        z_dict = self.gnn(x_dict, batch.edge_index_dict)\n",
        "        # Passiamo alla GNN e riceviamo gli embeddings aggiornati (z_dict)\n",
        "\n",
        "        # ⛔️ Gestione dei tensori vuoti per evitare errori nel summary\n",
        "        valid_z_list = []\n",
        "        for node_type, z in z_dict.items():\n",
        "            if z.size(0) != 0:\n",
        "               \n",
        "                valid_z_list.append(z.mean(dim=0, keepdim=True))\n",
        "\n",
        "        # Se nessun nodo ha embeddings validi, ritorniamo None\n",
        "        if len(valid_z_list) == 0:\n",
        "            #print(\"[DEBUG] Nessun embedding valido per il summary, ritorno None.\")\n",
        "            return None, None\n",
        "\n",
        "        # Calcolo del summary\n",
        "        summary = torch.cat(valid_z_list, dim=0).mean(dim=0, keepdim=True)\n",
        "        summary = torch.tanh(summary)\n",
        "\n",
        "        return z_dict, summary\n",
        "\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        batch: HeteroData,\n",
        "        entity_table: NodeType,\n",
        "    ) -> Tensor:\n",
        "        seed_time = batch[entity_table].seed_time\n",
        "        #takes the timestamp of the nodes for which we want to make predictions\n",
        "        #not the neighbours, but the nodes we want to make prediction for.\n",
        "        x_dict = self.encoder(batch.tf_dict)\n",
        "        #this creates a dictionar for all the nodes: each nodes has its\n",
        "        #embedding\n",
        "\n",
        "        rel_time_dict = self.temporal_encoder(\n",
        "            seed_time, batch.time_dict, batch.batch_dict\n",
        "        )\n",
        "        #this add the temporal information to the node using the\n",
        "        #HeteroTemporalEncoder\n",
        "\n",
        "        for node_type, rel_time in rel_time_dict.items():\n",
        "            x_dict[node_type] = x_dict[node_type] + rel_time\n",
        "        #add some other shallow embedder\n",
        "\n",
        "        for node_type, embedding in self.embedding_dict.items():\n",
        "            x_dict[node_type] = x_dict[node_type] + embedding(batch[node_type].n_id)\n",
        "\n",
        "        # for edge_type, edge_index in batch.edge_index_dict.items():\n",
        "        #     print(\"model edge_tipe: \", edge_type)\n",
        "        #     print(\"model edge_index: \", edge_index)\n",
        "        #print(\"model x_dict : \", x_dict['constructors'])\n",
        "\n",
        "        x_dict = self.gnn(\n",
        "            x_dict,#feature of nodes\n",
        "            batch.edge_index_dict,\n",
        "            batch.num_sampled_nodes_dict,\n",
        "            batch.num_sampled_edges_dict,\n",
        "        )#apply the gnn\n",
        "\n",
        "        return self.head(x_dict[entity_table][: seed_time.size(0)])#final prediction\n",
        "\n",
        "    def forward_dst_readout(\n",
        "        self,\n",
        "        batch: HeteroData,\n",
        "        entity_table: NodeType,\n",
        "        dst_table: NodeType,\n",
        "    ) -> Tensor:\n",
        "        if self.id_awareness_emb is None:\n",
        "            raise RuntimeError(\n",
        "                \"id_awareness must be set True to use forward_dst_readout\"\n",
        "            )\n",
        "        seed_time = batch[entity_table].seed_time\n",
        "        x_dict = self.encoder(batch.tf_dict)\n",
        "        # Add ID-awareness to the root node\n",
        "        x_dict[entity_table][: seed_time.size(0)] += self.id_awareness_emb.weight\n",
        "\n",
        "        rel_time_dict = self.temporal_encoder(\n",
        "            seed_time, batch.time_dict, batch.batch_dict\n",
        "        )\n",
        "\n",
        "        for node_type, rel_time in rel_time_dict.items():\n",
        "            x_dict[node_type] = x_dict[node_type] + rel_time\n",
        "\n",
        "        for node_type, embedding in self.embedding_dict.items():\n",
        "            x_dict[node_type] = x_dict[node_type] + embedding(batch[node_type].n_id)\n",
        "\n",
        "        x_dict = self.gnn(\n",
        "            x_dict,\n",
        "            batch.edge_index_dict,\n",
        "        )\n",
        "\n",
        "        return self.head(x_dict[dst_table])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MaL0--Sanwb"
      },
      "source": [
        "### Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "K87Ya4wPanwc"
      },
      "outputs": [],
      "source": [
        "def get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps, num_cycles=0.5):\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < warmup_steps:\n",
        "            return float(current_step) / float(max(1, warmup_steps))\n",
        "        progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
        "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * num_cycles * 2 * progress)))\n",
        "\n",
        "    return LambdaLR(optimizer, lr_lambda)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "SAHRIr15lVs6"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, scheduler) -> float:\n",
        "    model.train()\n",
        "\n",
        "    loss_accum = count_accum = 0\n",
        "    for batch in tqdm(loader_dict[\"train\"]):\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(\n",
        "            batch,\n",
        "            task.entity_table,\n",
        "        )\n",
        "        pred = pred.view(-1) if pred.size(1) == 1 else pred\n",
        "\n",
        "        loss = loss_fn(pred.float(), batch[entity_table].y.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        loss_accum += loss.detach().item() * pred.size(0)\n",
        "        count_accum += pred.size(0)\n",
        "\n",
        "    return loss_accum / count_accum\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, loader: NeighborLoader) -> np.ndarray:\n",
        "    model.eval()\n",
        "\n",
        "    pred_list = []\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        pred = model(\n",
        "            batch,\n",
        "            task.entity_table,\n",
        "        )\n",
        "        pred = pred.view(-1) if pred.size(1) == 1 else pred\n",
        "        pred_list.append(pred.detach().cpu())\n",
        "    return torch.cat(pred_list, dim=0).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "KMn-f8dmanwc"
      },
      "outputs": [],
      "source": [
        "def training_function(model, optimizer, epochs):\n",
        "    state_dict = None\n",
        "    best_val_metric = -math.inf if higher_is_better else math.inf\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_loss = train(model, optimizer)\n",
        "        val_pred = test(model, loader_dict[\"val\"])\n",
        "        #val_metrics = task.evaluate(val_pred, val_table)\n",
        "        val_metrics = custom_evaluate(val_pred, val_table, task.metrics)\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch: {epoch:02d}, Train loss: {train_loss}, Val metrics: {val_metrics}\")\n",
        "        #print(f\"Epoch: {epoch:02d}, Train loss: {train_loss}, Val metrics: {val_metrics}\")\n",
        "\n",
        "        if (higher_is_better and val_metrics[tune_metric] > best_val_metric) or (\n",
        "            not higher_is_better and val_metrics[tune_metric] < best_val_metric\n",
        "        ):\n",
        "            best_val_metric = val_metrics[tune_metric]\n",
        "            state_dict = copy.deepcopy(model.state_dict())\n",
        "\n",
        "\n",
        "    model.load_state_dict(state_dict)\n",
        "    val_pred = test(model, loader_dict[\"val\"])\n",
        "    val_metrics = custom_evaluate(val_pred, val_table, task.metrics)\n",
        "    print(f\"Best Val metrics for parameters {optimizer}, are: {val_metrics}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpFIqgwTanwd"
      },
      "source": [
        "## Cross validation cycle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "qLUoqMtranwd"
      },
      "outputs": [],
      "source": [
        "# #cross validation cycle:\n",
        "# #possible learning rates: [0.01, 0.001, 0.0001, 0.00001]\n",
        "# #possible batch sizes: [64, 256, 512]\n",
        "# #possible number of layers: [1, 2, 3]\n",
        "# #possible weight decay: [0.0001, 0.001, 0.01]\n",
        "\n",
        "# for lr in [0.01, 0.001, 0.0001, 0.00001]:#0.001\n",
        "#     #for batch_size in [64, 256, 512]:\n",
        "#         for num_layers in [1, 2, 3]:#1\n",
        "#             #for weight_decay in [0.0001, 0.001, 0.01]:\n",
        "#                 model = Model(\n",
        "#                     data=data,\n",
        "#                     col_stats_dict=col_stats_dict,\n",
        "#                     num_layers=num_layers,\n",
        "#                     channels=128,\n",
        "#                     out_channels=1,\n",
        "#                     aggr=\"sum\",\n",
        "#                     norm=\"batch_norm\",\n",
        "#                 ).to(device)\n",
        "#                 print(f\"Training with lr={lr}, num_layers={num_layers}\")\n",
        "#                 optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.001)\n",
        "#                 training_function(model, optimizer, epochs=10) # Set epochs to a smaller number for testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_RxdxPHanwd"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "yF3W68Eqlew_",
        "outputId": "a0c79d29-de33-48d4-bbf5-42039d061419"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, DGI Loss: nan\n",
            "Epoch 2, DGI Loss: nan\n",
            "Epoch 3, DGI Loss: nan\n",
            "Epoch 4, DGI Loss: nan\n",
            "Epoch 5, DGI Loss: nan\n",
            "Epoch 6, DGI Loss: nan\n",
            "Epoch 7, DGI Loss: nan\n",
            "Epoch 8, DGI Loss: nan\n",
            "Epoch 9, DGI Loss: nan\n",
            "Epoch 10, DGI Loss: nan\n",
            "Epoch 11, DGI Loss: nan\n",
            "Epoch 12, DGI Loss: nan\n",
            "Epoch 13, DGI Loss: nan\n",
            "Epoch 14, DGI Loss: nan\n",
            "Epoch 15, DGI Loss: nan\n",
            "Epoch 16, DGI Loss: nan\n",
            "Epoch 17, DGI Loss: nan\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-166-a338cffaecb0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtrain_dgi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_dgi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mafter_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-153-5d80250872d3>\u001b[0m in \u001b[0;36mtrain_dgi\u001b[0;34m(model, discriminator, optimizer, epochs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;31m# Ottenere le feature \"vere\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mz_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrain_dgi_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentity_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;31m# Corrompere il batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-157-76350a525b85>\u001b[0m in \u001b[0;36mpretrain_dgi_forward\u001b[0;34m(self, batch, entity_table)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpretrain_dgi_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mHeteroData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNodeType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mx_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_awareness_emb\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/relbench/modeling/nn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tf_dict)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mtf_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNodeType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorFrame\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     ) -> Dict[NodeType, Tensor]:\n\u001b[0;32m---> 83\u001b[0;31m         x_dict = {\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0mnode_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnode_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         }\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/relbench/modeling/nn.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     82\u001b[0m     ) -> Dict[NodeType, Tensor]:\n\u001b[1;32m     83\u001b[0m         x_dict = {\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mnode_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnode_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         }\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_frame/nn/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tf)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_frame/nn/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortcut\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"dropout probability has to be between 0 and 1, but got {p}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m     return (\n\u001b[0;32m-> 1425\u001b[0;31m         \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m     )\n\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_VF.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model = Model(\n",
        "    data=data,\n",
        "    col_stats_dict=col_stats_dict,\n",
        "    num_layers=1,\n",
        "    channels=128,\n",
        "    out_channels=1,\n",
        "    aggr=\"sum\",\n",
        "    norm=\"batch_norm\",\n",
        ").to(device)\n",
        "\n",
        "#copiamo i pesi del modello perché così possiamo vedere se stiamo effettivamente modificando model:\n",
        "initial_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "discriminator = DGIHead(128).to(device)\n",
        "\n",
        "#andiamo ad ottimizzare sia i parametri di model che del discriminatore\n",
        "optimizer_dgi = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(discriminator.parameters()),\n",
        "    lr=0.001,\n",
        "    weight_decay=1e-5,\n",
        ")\n",
        "\n",
        "train_dgi(model, discriminator, optimizer_dgi, loader_dict[\"train\"], device, entity_table=entity_table, epochs=20)\n",
        "\n",
        "after_state = model.state_dict()\n",
        "\n",
        "#verifica se sono cambiati\n",
        "changed_params = [k for k in initial_state if not torch.equal(initial_state[k], after_state[k])]\n",
        "print(\"Parametri modificati:\", changed_params if changed_params else \"Nessuno è cambiato\")\n",
        "\n",
        "\n",
        "#down stream\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
        "epochs = 100\n",
        "total_steps = epochs * len(loader_dict[\"train\"])\n",
        "warmup_steps = int(0.1 * total_steps)  # 10% warmup\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
        "state_dict = None\n",
        "best_val_metric = -math.inf if higher_is_better else math.inf\n",
        "\n",
        "#per mantenere la storia dei MAE nel tempo:\n",
        "val_metr_history = []\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss = train(model, optimizer, scheduler)\n",
        "    val_pred = test(model, loader_dict[\"val\"])\n",
        "    #val_metrics = task.evaluate(val_pred, val_table)\n",
        "    val_metrics = custom_evaluate(val_pred, val_table, task.metrics)\n",
        "\n",
        "    val_metr_history.append(val_metrics[tune_metric])\n",
        "\n",
        "    print(f\"Epoch: {epoch:02d}, Train loss: {train_loss}, Val metrics: {val_metrics}\")\n",
        "\n",
        "    if (higher_is_better and val_metrics[tune_metric] > best_val_metric) or (\n",
        "            not higher_is_better and val_metrics[tune_metric] < best_val_metric\n",
        "    ):\n",
        "        best_val_metric = val_metrics[tune_metric]\n",
        "        state_dict = copy.deepcopy(model.state_dict())\n",
        "\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "val_pred = test(model, loader_dict[\"val\"])\n",
        "val_metrics = custom_evaluate(val_pred, val_table, task.metrics)\n",
        "print(f\"Best Val metrics: {val_metrics}\")\n",
        "\n",
        "test_table = task.get_table(\"test\", mask_input_cols=False)\n",
        "test_pred = test(model,loader_dict[\"test\"])\n",
        "test_metrics = custom_evaluate(test_pred, test_table, task.metrics)\n",
        "print(f\"Best test metrics: {test_metrics}\")\n",
        "\n",
        "plot_validation_metrics([val_metr_history], [\"basic model\"],  metric_name=tune_metric)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkY-BYdoanwe"
      },
      "source": [
        "# Import a predefined model to use it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMAAXTPFanwe"
      },
      "outputs": [],
      "source": [
        "# model.load_state_dict(torch.load('best_model_GAT_head2.pth', map_location=torch.device('cpu')))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
